---
title             : "Reminder about Confidence Intervals"
shorttitle        : "CI REMINDER"

author: 
  - name          : "Marie Delacre"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "marie.delacre@ulb.ac.be"

affiliation:
  - id            : "1"
    institution   : "ULB"

authornote: |

abstract: |

keywords          : "keywords"
wordcount         : "X"
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("moments")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

### Introduction: How to compute a confidence interval around $\mu_1-\mu_2$ 

Considering the link between confidences intervals and NHST approach, we can think of confidence limits as the most extreme values of $\mu_1-\mu_2$ that we could define as null hypothesis and that would not lead to rejecting the null hypothesis [@Cumming_Finch_2001] (i.e that would be associated with a *p*-value that exactly equals $\frac{alpha}{2}$). 

Under the assumption of iid normal distributions of residuals with equal variances across groups, in order to test the null hypothesis that $\mu_1-\mu_2= (\mu_1-\mu_2)_0$, we can compute the following quantity:

\begin{equation} 
t_{Student}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
(\#eq:tstudent)
\end{equation} 

With $SE = \sigma_{pooled} \times \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$ and $\sigma_{pooled} = \sqrt{\frac{(n_1-1)*S^2_1+(n_2-1)*S^2_2}{n_1+n_2-2}}$. 

```{r, SAMPLMEANDIFF1, fig.cap= "Sampling distribution of Student's t under the assumptions of normality and homoscedasticity"}
samplingtest <- function(n1,n2,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=4)
    B <- rnorm(n2,mean=2)
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else {
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
  pooled_sd <- sqrt(((n1-1)*sd1^2+(n2-1)*sd2^2)/(n1+n2-2))
  pq <- (av-(4-2))/(pooled_sd*sqrt(1/n1+1/n2))
  
  1-pt(.025,df=n1+n2-2)
  plot(density(pq),main="Sampling distribution of t",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  legend("top",col=c("black","red"),legend=c("empirical distribution (based on simulations)","t-distribution with n1+n2-2 degrees of freedom"),lty=c(2,2),bty="n")
  
  df <- n1+n2-2
  sq <- seq(-4,4,.01) 
  lines(sq,dt(sq,df=df),lty=2,pch=9,cex=.5,col="red")
  
}

samplingtest(n1=50,n2=60,nsim=10)

```

Under the null hypothesis, this quantity will follow a central *t*- distribution with $n_1+n_2-2$ degrees of freedom (see Figure \ref{fig:SAMPLMEANDIFF1}) \footnote{Distribution is central because under the null hypothesis, the quantity is a (supposed normal) centered variable, divided by SE, an independant variable closely related with the $\chi^2$}. We can therefore easily define $(\mu_1-\mu_2)_L$, the lower limit of the confidence interval, such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}$ exactly equals the quantile  (1-$\frac{\alpha}{2}$) of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_L$, and the upper limit $(\mu_1-\mu_2)_U$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}$ exactly equals the quantile  $\frac{\alpha}{2}$ of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_U$:

\begin{equation} 
Pr[t_{n_1+n_2-2} \geq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit1)
\end{equation} 

\begin{equation} 
Pr[t_{n_1+n_2-2} \leq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit2)
\end{equation} 

```{r, SAMPLMEANDIFF2, fig.cap= "Sampling distribution of Welch's t under the assumptions of normality and heteroscedasticity"}
samplingtest2 <- function(n1,n2,sigma1,sigma2,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=4,sd=sigma1)
    B <- rnorm(n2,mean=2,sd=sigma2) 
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else { 
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
  pq <- (av-(4-2))/sqrt(sd1^2/n1+sd2^2/n2) 
  plot(density(pq),main="Sampling distribution of PQ",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  legend("top",col=c("black","red"),legend=c("empirical distribution (based on simulations)","t-distribution with df degrees of freedom"),lty=c(2,2),bty="n")

  df <- (sigma1^2/n1+sigma2^2/n2)^2/(((sigma1^2/n1)^2)/(n1-1)+((sigma2^2/n2)^2)/(n2-1)) 
  sq <- seq(-4,4,.01) 
  lines(sq,dt(sq,df=df),lty=2,pch=9,cex=.5,col="red")
  
}

samplingtest2(n1=50,n2=60, sigma1=2,sigma2=5,nsim=10)
```

Under the assumption of iid normal distributions of residuals with unequal variances across groups, in order to test the null hypothesis that $\mu_1-\mu_2= (\mu_1-\mu_2)_0$, we can compute the following quantity:

\begin{equation} 
t_{Welch}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
(\#eq:twelch)
\end{equation} 

With $SE = \sqrt{\frac{S^2_1}{n1}+\frac{S^2_2}{n2}}$. Again, under the null hypothesis, we know that this quantity will follow  a central *t*- distribution with $DF=\frac{(\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2})^2}{\frac{(\frac{S^2_1}{n_1})^2}{n_1-1}+\frac{(\frac{S^2_2}{n_2})^2}{n_2-1}}$ degrees of freedom. (see Figure \ref{fig:SAMPLMEANDIFF2}). We can therefore easily define $(\mu_1-\mu_2)_L$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}$ exactly equals the quantile  (1-$\frac{\alpha}{2}$) of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_L$, and the upper limit $(\mu_1-\mu_2)_U$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}$ exactly equals the quantile  $\frac{\alpha}{2}$ of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_U$:

\begin{equation} 
Pr[t_{DF} \geq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit1)
\end{equation} 

\begin{equation} 
Pr[t_{DF} \leq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit2)
\end{equation} 

It is not the most conventional way of computing confidences limits around any mean differences, but this approach is interesting as it helps to understand how to compute confidence limits around a measure of effect size.

### How to compute a confidence interval around Cohen's $\delta$

```{r, SAMPLMEANDIFF3, fig.cap= "Sampling distribution of centered mean difference divided by SE (in grey, i.e. pivotal quantity) and not centered mean difference divided by SE (in red), assuming normality and homoscedasticity."}
samplingtest3 <- function(n1,n2,mu1,mu2,sigma,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=mu1,sd=sigma)
    B <- rnorm(n2,mean=mu2,sd=sigma) 
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else { 
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
  pooled_sd <- sqrt(((n1-1)*sd1^2+(n2-1)*sd2^2)/(n1+n2-2))
  pq <- (av-(mu1-mu2))/(pooled_sd*sqrt(1/n1+1/n2)) # *1*: centered variable, divided by SE 
  plot(density(pq),xlim=c(-15,15),col="grey",main="Sampling distribution \n (not) centered variable divided by SE",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  
  # symmetric around 0
  lines(seq(-100,100,.01),dt(seq(-100,100,.01),df=n1+n2-2),lty=2)
  
  q2 <- (av)/(pooled_sd*sqrt(1/n1+1/n2)) # *2*: NOT centered variable, divided by SE
  lines(density(q2),lty=2,col="red",lwd=1.5) # plotting the sampling distribution of the pivotal quantity
  round(skewness(q2),3)  # not symmetric around mean(q2)
  
  pooled_sigma <-  sigma
  delta <- (mu1-mu2)/pooled_sigma
  NCP <- delta*sqrt((n1*n2)/(n1+n2))
  
  lines(density(rt(1000000,df=n1+n2-2,ncp=NCP)),lty=2)
  
  legend("top",col=c("grey","red"),legend=c(paste0("centered variable divided by SE (skewness=",round(skewness(pq),3),")"),paste0("not centered variable divided by SE (skewness=",round(skewness(q2),3),")")),lty=c(2,2),bty="n")
}

samplingtest3(n1=5,n2=12, mu1=6,mu2=2,sigma=5,nsim=10)
```

We previously mentioned that if the null hypothesis is true,$t_{Student}$ (see equation \@ref(eq:tstudent)) will follow a central *t*-distribution. However, if the null hypothesis is false, the distribution of this quantity will not be centered, and noncentral *t*-distribution will arise [@Cumming_Finch_2001], as illustrated in Figure \ref{fig:SAMPLMEANDIFF3}. 

Noncentral *t*-distributions are described by two parameters: degrees of freedom (df) and noncentrality parameter [that we will call $\Delta$; @Cumming_Finch_2001], the last being a function of $\delta$ and sample sizes $n_1$ and $n_2$:
 
\begin{equation}
\Delta = \frac{\mu_1-\mu_2}{\sigma_{pooled}} \times \sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}
(\#eq:ncp)
\end{equation} 

Considering the link between $\Delta$ and $\delta$, it is possible to compute confidence limits for $\Delta$, and divide them by $\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}$ in order to have confidence limits for $\delta$. In other word, we first need to determine the noncentrality parameters of the *t*-distributions for which $t_{Student}$ corresponds respectively to  the $1-\frac{\alpha}{2}$ and to the $\frac{\alpha}{2}$ th. quantile: 

$$P[t_{df, \Delta_L} \geq t_{Student}] = \frac{\alpha}{2} $$ 

$$P[t_{df, \Delta_U} \leq t_{Student}] = \frac{\alpha}{2} $$ 

With $df = n_1+n_2-2$. Second, we divide $\Delta_L$ and $\Delta_U$ by $\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}$ in order to define $\delta_L$ and $\delta_U$: 

$$\delta_L = \frac{\Delta_L}{\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}}$$

$$\delta_U = \frac{\Delta_U}{\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}}$$

# How to determine the confidence interval around Shieh's $\delta*$

```{r, include=FALSE}
samplingtest4 <- function(n1,n2,mu1,mu2,sigma1,sigma2,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=mu1,sd=sigma1)
    B <- rnorm(n2,mean=mu2,sd=sigma2) 
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else { 
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
  pq <- (av-(mu1-mu2))/sqrt(sd1^2/n1+sd2^2/n2)
  plot(density(pq),xlim=c(-15,15),col="grey",main="Sampling distribution \n (not) centered variable divided by SE",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  
  DF <- (sd1^2/n1 + sd2^2/n2)^2 / ((sd1^2/n1)^2/(n1-1) + (sd2^2/n2)^2/(n2-1))  
  # symmetric around 0
  
  DF_theo <- (sigma1^2/n1 + sigma2^2/n2)^2 / ((sigma1^2/n1)^2/(n1-1) + (sigma2^2/n2)^2/(n2-1))  
  lines(seq(-100,100,.01),dt(seq(-100,100,.01),df=DF_theo),lty=2)
  
  q2 <- (av)/sqrt(sd1^2/n1+sd2^2/n2) # *2*: NOT centered variable, divided by SE
  lines(density(q2),lty=2,col="red",lwd=1.5) # plotting the sampling distribution of the pivotal quantity
  round(skewness(q2),3)  # not symmetric around mean(q2)

  delta <- (mu1-mu2)/sqrt(sigma1^2/(n1/(n1+n2))+sigma2^2/(n2/(n1+n2))) # theoretical shieh
  NCP <- delta*sqrt(n1+n2)
  
#  lines(density(rt(1000000,df=DF_theo,ncp=NCP)),lty=2)
  
  legend("top",col=c("grey","red"),legend=c(paste0("centered variable divided by SE (skewness=",round(skewness(pq),3),")"),paste0("not centered variable divided by SE (skewness=",round(skewness(q2),3),")")),lty=c(2,2),bty="n")
}
```

Like $t_{Student}$, $t_{Welch}$ (see equation \@ref(eq:twelch)) will follow a central *t*-distribution only if the null hypothesis is true. If the null hypothesis is false, it will follow a noncentral *t*-distribution, as illustrated in Figure \ref{fig:SAMPLMEANDIFF4}. 

```{r, SAMPLMEANDIFF4, fig.cap= "Sampling distribution of centered mean difference divided by SE (in grey, i.e. pivotal quantity) and not centered mean difference divided by SE (in red), assuming normality and homoscedasticity."}

samplingtest4(n1=5,n2=12, mu1=6,mu2=2,sigma1=5,sigma2=5,nsim=10)

```

The noncentrality parameter $\Delta*$ is a function of $\delta*$ and total sample size $N = n_1 + n_2$ [@Shieh_2013]
 
\begin{equation}
\Delta* = \frac{\mu_1-\mu_2}{\sqrt{\frac{\sigma_1^2}{n_1/N}+\frac{\sigma_2^2}{n_2/N}}} \times \sqrt{N}
(\#eq:ncp)
\end{equation} 

Considering the link between $\Delta$ and $\delta$, we can compute confidence limits for $\Delta*$, and divide them by $\sqrt{N}$ in order to have confidence limits for $\delta*$. We first need to determine the noncentrality parameters of the distributions for which $t_{Welch}$ corresponds respectively to  the $1-\frac{\alpha}{2}$ and to the $\frac{\alpha}{2}$ th. quantile. 
$$P[t_{v, \Delta*_L} \geq t_{Welch}] = \frac{\alpha}{2} $$ and 
$$P[t_{v, \Delta*_U} \leq t_{Welch}] = \frac{\alpha}{2} $$. 

With *v* approximated by $\hat{v} = \frac{(\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2})^2}{\frac{(\frac{S_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{S_2^2}{n_2})^2}{n_2-1}}$ [@Shieh_2013]

Second, we divide $\Delta*_L$ and $\Delta*_U$ by $\sqrt{N}$ in order to have $\delta*_L$ and $\delta*_U$ (i.e. confidences limits for Shieh's $\delta*$). 



