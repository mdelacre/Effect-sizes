---
title             : "Mathematical study of Glass's d"
shorttitle        : "Glass's d"

author: 
  - name          : "Marie Delacre" 
    affiliation   : "1"
    corresponding : yes    
    address       : "CP191, avenue F.D. Roosevelt 50, 1050 Bruxelles"
    email         : "marie.delacre@ulb.ac.be"

affiliation:
  - id            : "1"
    institution   : "Universit√© Libre de Bruxelles, Service of Analysis of the Data (SAD), Bruxelles, Belgium"

authornote: |
  
  I would like to thank Matt Williams and Thom Baguley for their helpful insights in order to undertand the phenomenon explained in this appendix.

abstract: |

  
keywords          : "keywords"
wordcount         : "X"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("PearsonDS")
library("moments")
```

```{r glass test,echo=FALSE}
nSims=10000
test=function(sd,nSims=10000,m1,m2,n,skew,kurt=95.75,title,ylim){
   
   glasspos1<-rep(0,nSims)
   glasspos2<-rep(0,nSims)
   
   sd1<-rep(0,nSims)
   sd2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   for (i in 1:nSims){

      y1 <- rpearson(n,moments=c(m1,sd^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
      y2 <- rpearson(n,moments=c(m2,sd^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  

      sd1[i] <- sd(y1)
      sd2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
      glasspos1[i] <- (mean(y1)-mean(y2))/sd(y1)
      glasspos2[i] <- (mean(y1)-mean(y2))/sd(y2)
      
   }
   
   plot(density(glasspos1),col="blue",lty=1,xlim=c(-10,10),main=title,xlab="glass's ds",ylim=ylim)
   lines(density(glasspos2),col="red",lty=2)
   
   #legend("topright",legend=c("glass with sd1","glass with sd2"),lty=c(1,2),col=c("blue","red"),bty="n")
   return(cbind(sd1,sd2,meandiff,glasspos1,glasspos2,mean1,mean2))
}
```

# When two samples are extracted from distributions with identical shapes, with **$\sigma_1= \sigma_2$** and **$n_1=n_2$**

When population distributions are symmetric (i.e. $\gamma_1=0$), the sampling distribution of glass's $d_s$ is the same, whatever one chooses $s_1$ or $s_2$ as standardizer. As an example, in Figure \ref{fig:glass1}, we plotted the sampling distribution of both measures of glass's $d_s$ when two samples of 20 subjects are extracted from two symmetric distributions where $\gamma_1=0$,$\gamma_2=95.75$, $\sigma_1=\sigma_2=1$ and $\mu_2=0$. $\mu_1$ is either 0 or 1, depending on the plot. One can see that in the two plots, distributions of glass's $d_S$ using $s_1$ and $s_2$ as standardiser are superimposed. \footnote{Anytime the mean difference is null, the sampling distribution of glass will be symmetric. On the other side, anytime the mean difference is non null, the sampling distriution of glass's ds will be skewed: right-skewed if the mean difference is positive, left-skewed if the mean difference is negative. The right skewed distribution is therefore only due to the sense of the mean difference} 

```{r glass1, fig.cap = "Comparison of Glass's ds when choosing either s1 (blue line) or s2 (red dotted line) as standardizer, with s1=standard deviation of the first sample and s2=standard deviation of the second sample, when n1=n2=20 and both samples are extracted from a distribution where G1 =0, G2=95.75 and sigma=1", echo=FALSE}
par(mfrow=c(1,2),mar=c(2,2,7,2))
A=test(sd=1,m1=0,m2=0,skew=0,n=20,title="mu1-mu2=0",ylim=c(0,1.2)) # symmetric distribution
B=test(sd=1,m1=1,m2=0,skew=0,n=20,title="mu1-mu2=1",ylim=c(0,1.2)) # symmetric distribution
par(xpd=TRUE)
```

However, when population distributions are skewed (i.e. $\gamma_1 \neq 0$), the sampling distribution of glass's $d_s$ varies as a function of the chosen standardizer, as illustrated in Figure \ref{fig:glass2}. 

```{r glass2, fig.cap = "Comparison of Glass's ds when choosing either sd1 (blue line) or sd2 (red dotted line) as standardizer when n1=n2=20 and both samples are extracted from a distribution where sigma=1, G2=95.75, G1 is either -6.32 (left) or 6.32 (right). In all cases, the second sample is extracted from a population distribution where mu2=0. First sample is extracted from a population distribution where mu1 is either 0 (top) of 1 (bottom)", echo=FALSE}
par(mfrow=c(2,2),mar=c(2,2,7,2))
mu1=1
mu2=0
C=test(sd=1,m1=0,m2=0,skew=-6.32,n=20,title="Left-skewed distributions \n mu1-mu2=0",ylim=c(0,1.5)) # symmetric distribution
D=test(sd=1,m1=0,m2=0,skew=6.32,n=20,title="Right-skewed distributions \n mu1-mu2=0",ylim=c(0,1.5)) # symmetric distribution
E=test(sd=1,m1=mu1,m2=mu2,skew=-6.32,n=20,title="Left-skewed distributions \n mu1-mu2=1",ylim=c(0,.8)) # symmetric distribution
F=test(sd=1,m1=mu1,m2=mu2,skew=6.32,n=20,title="Right-skewed distributions \n mu1-mu2=1",ylim=c(0,.8)) # symmetric distribution
```

It might seem surprising, or even counter-intuitive, as in all plots, $s_1$ and $s_2$ are both estimates of the same population standard deviation ($\sigma$), based on the same number of observations (as $n_1=n_2$), but this phenomenon can be mathematically explained. In the following section, we will provides detailed informations to understand the results plotted in Figure \ref{fig:glass2}.

## When both samples are extracted from a common right-skewed distribution (**$\mu_1-\mu_2=0$**) (top right plot in Figure \ref{fig:glass2})

We will first study the configuration where both samples are extracted from a right-skewed distribution where $\mu=0$, $\sigma=1$, $\gamma_1=6.32$ and $\gamma_2=95.75$. Because this distribution is right-skewed, the sampling distributions of $\bar{X_1}$ and $\bar{X_2}$ will also be right-skewed. However, because $\bar{X_1}$ and $\bar{X_2}$ are identically distributed, $\bar{X_1}-\bar{X_2}$ will follow a symmetric distribution, as illustrated in Figure \ref{fig:sampldist1} (right plot).
More specifically, the distribution will be symmetrically centered around $\mu_1-\mu_2=0$ (i.e. the green area in the right plot in Figure \ref{fig:sampldist1} is the reflexion of the blue area, with the vertical line being the line of reflexion). It means that:  
- Half the mean difference estimates will be positive (i.e. $\bar{X_1}-\bar{X_2} > 0$; see green area ) and the other half will be negative (i.e. $\bar{X_1}-\bar{X_2} < 0$; see blue area).  
- For a constant k, $|(\mu_1-\mu_2)-k|=|(\mu_1-\mu_2)+k|$

Because we compute the mean difference as the mean estimate of the first sample minus the mean estimate of the second sample, there is a positive correlation between $\bar{X_1}$ and $\bar{X_1}-\bar{X_2}$, and a negative correlation between $\bar{X_2}$ and $\bar{X_1}-\bar{X_2}$ (correlations would be trivially reversed if we computed $\bar{X_2}-\bar{X_1}$ instead of $\bar{X_1}-\bar{X_2}$).

```{r sampldist1, fig.cap = "Sampling distribution of m1 (blue line in left plot), m2 (red dotted line in left plot), and m1-m2 (right plot), when m1 and m2 are estimates of the mean of a population distribution where mu=0, sigma=1,G1=6.32 and G2=95.75, with n1=n2=20", echo=FALSE}
par(mfrow=c(1,2),mar=c(2,2,7,2))
plot(density(D[,6]),col="blue",main="sampling distribution \n of m1 and m2",ylim=c(0,2.5)) # sd1
lines(density(D[,7]),col="red",lty=2) # sd1

plot(density(D[,3]),main="sampling distribution \n of m1-m2",ylim=c(0,2)) # m1
abline(v=0) 
Dens=density(D[,3])
polygon(c(0,Dens$x[Dens$x<=0]),c(Dens$y[Dens$x<=0],0), col="lightblue")
polygon(c(Dens$x[Dens$x>=0],0),c(Dens$y[Dens$x>=0],0), col="lightgreen")
``` 

The sampling distributions of $s_1$ and $s_2$ are right-skewed, because estimates of the standard deviation are bounded: they can be very large, but never below 0. Moreover, as $s_1$ and $s_2$ are estimates of the same population standard deviation $\sigma$, based on the same sample size, of course, the sampling distributions of $s_1$ and $s_2$ will be identical, as illustrated in Figure \ref{fig:sampldist2}. 

```{r sampldist2, fig.cap = "Sampling distribution of s1 (blue line) and s2 (red dotted line), when s1 and s2 are estimates of the standard deviation of a population distribution where mu=0, sigma=1,G1=6.32 and G2=95.75, with n1=n2=20", echo=FALSE}
par(mfrow=c(1,1),mar=c(2,2,7,2))
plot(density(D[,1]),col="blue",main="sampling distribution \n of s1 and s2",ylim=c(0,1.2)) # sd1
lines(density(D[,2]),col="red",lty=2) # sd1
```

Therefore, how to explain the different sampling distributions of glass's $d_s$, as a function of the standardizer? This is due to the fact that when distributions are skewed, there is a non-nul correlation between $\bar{X}$ and s (see Zhang, 2007). More specifically, when distributions are right-skewed, there is a **positive** correlation between $\bar{X}$ and s, as illustrated in the left plots in Figure \ref{fig:meanvssd1}.

```{r meanvssd1, fig.cap = "sample mean against sample standard deviation (left) and sample mean difference against sample standard deviation (right), for 100,000 iterations of sampling from two population distributions where mu=0, sigma=1,G1=6.32 and G2=95.75, with n1=n2=20", echo=FALSE}
par(mfrow=c(2,2),mar=c(5,4,1,1))
plot(D[,6],D[,1],xlab=expression(bar(X)[1]),ylab=expression(SD[1]),pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
plot(D[,3],D[,1],xlab=expression(bar(X)[1]-bar(X)[2]),ylab="",pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
plot(D[,7],D[,2],xlab=expression(bar(X)[2]),ylab=expression(SD[2]),pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
plot(D[,3],D[,2],xlab=expression(bar(X)[1]-bar(X)[2]),ylab="",pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
```

First, consider the glass's $d_s$ estimate using $s_1$ as standardiser. We already mentioned that there is a *positive* correlation between $\bar{X_1}$ and $\bar{X_1}-\bar{X_2}$ ($cor(\bar{X_1},\bar{X_1}-\bar{X_2})>0$). Because there is also a positive correlation between $\bar{X_1}$ and $s_1$ ($cor(\bar{X_1},s_1)>0$), it results in a **positive** correlation between $\bar{X_1}-\bar{X_2}$ and $s_1$ ($cor(\bar{X_1}-\bar{X_2},s_1)>0$; see top right plot in Figure \ref{fig:meanvssd1}): when moving from the left to the right in the right plot in Figure \ref{fig:sampldist1}, $s_1$ get larger. 

As a consequence, for many positive numbers k, $-k$ will be divided by a smaller positive value (resulting in a larger ratio) than $+k$, resulting in a left-skewed sampling distribution of glass's $d_S$.\footnote{This is not true for each value of k, because the correlation between the mean difference and standard deviation estimates is not perfect, but the larger k (i.e. the more extreme the mean difference estimate), the larger the probability that it is true.} Importantly, while the median of the sampling distribution of glass's $d_s$ is `r round(median(D[,4],2))`, as expected (because the sampling distributions of $\bar{X_1}-\bar{X_1}$ is centered around 0),  the mean will be a little lower (i.e. `r round(mean(D[,4]),2)`), meaning that glass's $d_s$ is negatively biased.

When considering $s_2$ as standardiser, because there is a *negative* correlation between $\bar{X_2}$ and $\bar{X_1}-\bar{X_2}$, there is also a **negative** correlation between $\bar{X_1}-\bar{X_2}$ and $s_2$ (see bottom right plot in Figure \ref{fig:meanvssd1}): when moving from the left to the right in the right plot in Figure \ref{fig:sampldist1}, $s_2$ get lower. In other word, for many positive numbers k, $-k$ will be divided by a larger positive value (resulting in a larger ratio) than $+ k$, resulting in a right-skewed sampling distribution of glass's $d_S$.  This time, while the median of the sampling distribution of glass's $d_s$ is still `r round(median(D[,5],2))`, the mean will be a little larger (i.e. `r round(mean(D[,5]),2)`), meaning that glass's $d_s$ is positively biased. 

## When both samples are extracted from a common left-skewed distribution (**$\mu_1-\mu_2=0$**) (top left plot in Figure \ref{fig:glass2})

When distributions are left-skewed, one observes the opposite: there is a **negative** correlation between $\bar{X}$ and s, as illustrated in the left plots in Figure \ref{fig:meanvssd2}.

```{r meanvssd2, fig.cap = "sample mean against sample standard deviation (left) and sample mean difference against sample standard deviation (right), for 100,000 iterations of sampling from two population distributions where mu=0, sigma=1,G1=-6.32 and G2=95.75, with n1=n2=20", echo=FALSE}
par(mfrow=c(2,2),mar=c(5,4,1,1))
plot(C[,6],C[,1],xlab=expression(bar(X)[1]),ylab=expression(SD[1]),pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
plot(C[,3],C[,1],xlab=expression(bar(X)[1]-bar(X)[2]),ylab="",pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
plot(C[,7],C[,2],xlab=expression(bar(X)[2]),ylab=expression(SD[2]),pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
plot(C[,3],C[,2],xlab=expression(bar(X)[1]-bar(X)[2]),ylab="",pch=19,cex=.5) # sd1
abline(v=0,lty=2,col="red")
```

Therefore, when moving from the left to the right in the right plot in Figure \ref{fig:sampldist1}, $s_1$ get lower ($cor(\bar{X_1},s_1) < 0 \; and \; cor(\bar{X_1},\bar{X_1}-\bar{X_2}>0)  \rightarrow cor(\bar{X_1}-\bar{X_2},s_1)<0$) and $s_2$ get larger ($cor(\bar{X_2},s_2) < 0 \; and \; cor(\bar{X_2},\bar{X_1}-\bar{X_2}<0)  \rightarrow cor(\bar{X_1}-\bar{X_2},s_2)>0$). As a consequence, glass's $d_S$ will be positively biased when using $s_1$ as a standardiser, and negatively biased when using $s_2$ as a standardiser.

## When samples are extracted from skewed distributions, with **$\mu_1-\mu_2=1$** (bottom plot in Figure \ref{fig:glass2})

We will first consider the example where both samples are extracted from right-skewed distributions with $\mu_1$ and $\mu_2$ being respectively `r mu1` and `r mu2`, and other moments of the population distributions being equal: $\sigma=1$, $\gamma_1=6.32$ and $\gamma_2=95.75$ (see bottom right plot in Figure \ref{fig:glass2}). 

Of course, the sampling distributions of $\bar{X_1}$ and $\bar{X_2}$ are not superimposed anymore, because $\bar{X_1}$ will be centered around $\mu_1=1$, and $\bar{X_2}$ will be centered around $\mu_2=0$. However, except for the mean, all other moments of both distributions (i.e. $\gamma_1$, $\gamma_2$ and $\sigma$) remain identical (see left plot in Figure \ref{fig:sampldist4}) and therefore, the sampling distribution of $\bar{X_1}-\bar{X_2}$ still follow  a symmetric distribution, as illustrated in the right plot in Figure \ref{fig:sampldist4}. 

```{r sampldist4, fig.cap = "Sampling distribution of m1 (blue line in left plot), m2 (red dotted line in left plot), and m1-m2 (right plot), when m1 is the mean of a sample extracted from a population distribution where mu=1 and m2 is the mean of a sample extracted from a population distribution where mu2=0. Except for the mean, all other moments of both populations distributions are identical, i.e. sigma=1,G1=6.32 and G2=95.75, with n1=n2=20", echo=FALSE}
par(mfrow=c(1,2),mar=c(2,2,7,2))
plot(density(F[,6]),col="blue",main="sampling distribution \n of m1 and m2",ylim=c(0,2.5),xlim=c(-1,3)) # sd1
lines(density(F[,7]),col="red",lty=2) # sd1

plot(density(F[,3]),main="sampling distribution \n of m1-m2",ylim=c(0,2),xlim=c(-1,3)) # m1
abline(v=0,lty=2) 
Dens=density(F[,3])
polygon(c(0,Dens$x[Dens$x<=0]),c(Dens$y[Dens$x<=0],0), col="lightblue")
polygon(c(Dens$x[Dens$x>=0],0),c(Dens$y[Dens$x>=0],0), col="lightgreen")
``` 

When $\mu_1-\mu_2$ was nul, comparing the magnitude of glass's $d_S$ when $\bar{X_1}-\bar{X_2} = (\mu_1-\mu_2) \pm k$ was only a function of the denominator (as $|(\mu_1-\mu_2)-k|=|(\mu_1-\mu_2)+k|$). When $\mu_1-\mu_2 \neq 0$, it is a function of both numerator and denominator. For example, when $\mu_1-\mu_2=1$, only about `r round(sum(F[,3]<0)/length(F[,3])*100,2)`% of the mean estimates are negative, meaning that almost all mean difference estimates will be positive (so will be glass's $d_s$ estimates). ATTENTION, BIEN SUR POUR AVOIR PRESQUE TTE LES DIFF POSITIVES IL FAUT QUE LA DIFF DE MOYENNE SOIT AU MOINS AUSSI GRANDE QUE SIGMA... SI SIGMA EST BCP PLUS GRAND QUE LA DIFF DE MOENNE ON AURA QD MM ENCORE PLEINS D ESTIMATIONS NEGATIVES!When computing glass's $d_s$ using $s_1$ as standardizer, the mean difference estimates that are close of 0 will be divided by a smaller standard deviation estimate that larger mean difference estimates (as $cor(\bar{X_1}-\bar{X_2},s_1)>0$). On the other side, when computing glass's $d_s$ using $s_2$ as standardizer, the mean difference estimates that are very small will be divided by a larger standard deviation estimate than large mean difference estimates (as $cor(\bar{X_1}-\bar{X_2},s_2)<0$). It is therefore not surprising that the sampling distribution of glass's $d_s$ is more skewed and variable when using $s_2$ rather than $s_1$ as standardizer.\footnote{Of course, when the mean difference is negative, this is the opposite: the sampling distribution of glass's $d_s$ will be left-skewed, and will be more skewed and variable when using $s_1$ rather than $s_2$ as standardizer}. When distributions are extracted from a left-skewed distribution (bottom left in Figure \ref{fig:glass2}), this is exactly the opposite.

# When two samples are extracted from distributions with identical shapes, and $n_1 \neq n_2$

```{r glass test2,echo=FALSE}
test2=function(sd,nSims=10000,m1,m2,n1,n2,skew,kurt=95.75,title,ylim,verbose=FALSE){
   
   glasspos1<-rep(0,nSims)
   glasspos2<-rep(0,nSims)
   
   sd1<-rep(0,nSims)
   sd2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   for (i in 1:nSims){

      y1 <- rpearson(n1,moments=c(m1,sd^2,skewness=skew*(n1-2)/sqrt(n1*(n1-1)),kurtosis=(kurt*(n1-2)*(n1-3)-6*(n1-1))/(n1^2-1)+3))       
      y2 <- rpearson(n2,moments=c(m2,sd^2,skewness=skew*(n2-2)/sqrt(n2*(n2-1)),kurtosis=(kurt*(n2-2)*(n2-3)-6*(n2-1))/(n2^2-1)+3))  

      sd1[i] <- sd(y1)
      sd2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
      glasspos1[i] <- (mean(y1)-mean(y2))/sd(y1)
      glasspos2[i] <- (mean(y1)-mean(y2))/sd(y2)
      
   }
   
   if (verbose==FALSE){
   plot(density(glasspos1),col="blue",lty=1,xlim=c(-10,10),main=title,xlab="glass's ds",ylim=ylim)
   lines(density(glasspos2),col="red",lty=2)
   }
   
   #legend("topright",legend=c("glass with sd1","glass with sd2"),lty=c(1,2),col=c("blue","red"),bty="n")
   return(cbind(sd1,sd2,meandiff,glasspos1,glasspos2,mean1,mean2))
}
```

When population distributions are symmetric (i.e. $\gamma_1=0$), the sampling distribution of glass's $d_s$ is only a function of the sample size of the group from which standardizer is computed (because $\sigma_1=\sigma_2$). Using the SD of the smallest group as standardiser results in a more biased and variable measure of Glass's $d_S$, whatever it is the first or the second group, as illustrated in Figure \ref{fig:glass3}.

```{r glass3, fig.cap = "Comparison of Glass's ds when choosing either sd1 or sd2 as standardizer when both samples are extracted from a distribution where sigma=1, m1=1,m2=0, G1=0 and G2=95.75, and either n1=100 and n2=20, or n1=20 and n2=100", echo=FALSE}
par(mfrow=c(1,2),mar=c(2,2,7,2))
mu1=1
mu2=0
G=test2(sd=1,m1=mu1,m2=mu2,skew=0,kurt=95.75,n1=100,n2=20,title="n1=100 and n2=20",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution
H=test2(sd=1,m1=mu1,m2=mu2,skew=0,kurt=95.75,n1=20,n2=100,title="n1=20 and n2=100",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution

plot(1,col="white",main="",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=c(expression(paste(sigma, "= SD1 (when n1 =100)")),expression(paste(sigma, "= SD2 (when n2 = 20)")),expression(paste(sigma, "= SD1 (when n1 = 20)")),expression(paste(sigma, "= SD2 (when n2 = 100)"))),col=c("black","orange","blue","green"),bty="n",lty=c(1,2,2,2))

plot(density(G[,4]),col="black",main="",lwd=2)
lines(density(G[,5]),col="orange",lwd=2)
lines(density(H[,4]),col="blue",lwd=2,lty=2)
lines(density(H[,5]),col="green",lwd=2,lty=2)
```

We know that the sampling distribution of $\bar{X_1}-\bar{X_2}$ is always symmetric, but that the sampling distribution of any standard deviation is right-skewed (because a standard deviation can never be below 0). The lower the sample size, the more skewed the sampling distribution of the standard deviation and the larger it's variance. As a consequence, the more skewed the sampling distribution of glass's $d_s$).

When distributions are skewed, the skewness of the glass's $d_s$ distribution (and therefore, the bias and variance of glass's $d_s$) depends on two parameters: the size of the group from which standardizer is computed (as with symmetric distributions), but also the correlation between $\bar{X_1}-\bar{X_2}$ and $s_j$. As long as $\mu_1-\mu_2$ is positive (or negative), the more skewed and variable estimation of glass's $d_s$ will occur when $s_j$ is simultaneously negatively (or positively) correlated with $\bar{X_1}-\bar{X_2}$ and computed based on the smallest sample size. On the other side, the best estimation (less biased and variable) will occur when $s_j$ is simultaneously positively (or negatively) correlated with $\bar{X_1}-\bar{X_2}$ and computed based on the largest sample size. This is illustrated in Figure \ref{fig:glass4} in four plots where samples are extracted from either a right-skewed distribution ($\gamma_1=6.32$; right) or a left-skewed distribution ($\gamma_1=-6.32$; left), and with a positive (top) or negative (bottom) population mean difference. 

```{r legend, echo=FALSE}
plot(1,col="white",main="",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=c(expression(paste(sigma, "= SD1 (when n1 =100)")),expression(paste(sigma, "= SD2 (when n2 = 20)")),expression(paste(sigma, "= SD1 (when n1 = 20)")),expression(paste(sigma, "= SD2 (when n2 = 100)"))),col=c("black","orange","blue","green"),bty="n",lty=c(1,2,2,2))
```

```{r glass4, fig.cap = "Comparison of Glass's ds when choosing either sd1 or sd2 as standardizer when both samples are extracted from a distribution where sigma=1, G2=95.75 and G1 is either +6.32 (right) or -6.32 (left), and m1-m2 is either 1 (top) or -1 (bottom). Sample sizes are either n1=100 and n2=20, or n1=20 and n2=100", echo=FALSE}

mu1=1
mu2=0
I=test2(sd=1,m1=mu1,m2=mu2,skew=-6.32,kurt=95.75,n1=100,n2=20,title="n1=100 and n2=20",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution
J=test2(sd=1,m1=mu1,m2=mu2,skew=-6.32,kurt=95.75,n1=20,n2=100,title="n1=20 and n2=100",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution

K=test2(sd=1,m1=mu1,m2=mu2,skew=6.32,kurt=95.75,n1=100,n2=20,title="n1=100 and n2=20",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution
L=test2(sd=1,m1=mu1,m2=mu2,skew=6.32,kurt=95.75,n1=20,n2=100,title="n1=20 and n2=100",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution

M=test2(sd=1,m1=mu2,m2=mu1,skew=-6.32,kurt=95.75,n1=100,n2=20,title="n1=100 and n2=20",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution
N=test2(sd=1,m1=mu2,m2=mu1,skew=-6.32,kurt=95.75,n1=20,n2=100,title="n1=20 and n2=100",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution

O=test2(sd=1,m1=mu2,m2=mu1,skew=6.32,kurt=95.75,n1=100,n2=20,title="n1=100 and n2=20",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution
P=test2(sd=1,m1=mu2,m2=mu1,skew=6.32,kurt=95.75,n1=20,n2=100,title="n1=20 and n2=100",ylim=c(0,1.5),verbose=TRUE) # symmetric distribution

par(mfrow=c(2,2),mar=c(1,1,1,1))
plot(density(I[,4]),col="black",main="",lwd=2,xlab="")
lines(density(I[,5]),col="orange",lwd=2)
lines(density(J[,4]),col="blue",lwd=2,lty=2)
lines(density(J[,5]),col="green",lwd=2,lty=2)

plot(density(K[,4]),col="black",main="",lwd=2,xlab="")
lines(density(K[,5]),col="orange",lwd=2)
lines(density(L[,4]),col="blue",lwd=2,lty=2)
lines(density(L[,5]),col="green",lwd=2,lty=2)

plot(density(M[,4]),col="black",main="",lwd=2,xlab="")
lines(density(M[,5]),col="orange",lwd=2)
lines(density(N[,4]),col="blue",lwd=2,lty=2)
lines(density(N[,5]),col="green",lwd=2,lty=2)

plot(density(O[,4]),col="black",main="",lwd=2,xlab="")
lines(density(O[,5]),col="orange",lwd=2)
lines(density(P[,4]),col="blue",lwd=2,lty=2)
lines(density(P[,5]),col="green",lwd=2,lty=2)
```
In the two top plots in Figure \ref{fig:glass4}, $\bar{X_1}-\bar{X_2}$ is positive. When distributions are right-skewed (top right), choosing $s_2$ (i.e. the $SD$ that is negatively correlated with $\bar{X_1}-\bar{X_2}$) associated with the smallest sample size (i.e. $n_2=20$) will result in the most biased and variable estimation of glass's $d_s$. The best estimation will occur when choosing $s_1$ associated with the largest sample size (i.e. $n_1=100$). On the other side, when distributions are left-skewed (top left), choosing $s_1$ (i.e. the $SD$ that is negatively correlated with $\bar{X_1}-\bar{X_2}$) associated with the smallest sample size  (i.e. $n_1=20$) will result in the most biased and variable estimation of glass's $d_s$. The less biased and variable glass $d_s$ will occur when $s_2$ is chosen as standardizer and $n_2=100$.

In the two bottom plots in Figure \ref{fig:glass4}, $\bar{X_1}-\bar{X_2}$ is negative. When distributions are right-skewed (bottom right), choosing $s_1$ (i.e. the $SD$ that is positively correlated with $\bar{X_1}-\bar{X_2}$) associated with the smallest sample size (i.e. $n_1=20$) will result in the most biased and variable estimation of glass's $d_s$. The best estimation will occur when choosing $s_2$ associated with the largest sample size (i.e. $n_2=100$). On the other side, when distributions are left-skewed (top left), choosing $s_2$ (i.e. the $SD$ that is positively correlated with $\bar{X_1}-\bar{X_2}$) associated with the smallest sample size  (i.e. $n_2=20$) will result in the most biased and variable estimation of glass's $d_s$. The less biased and variable glass $d_s$ will occur when $s_1$ is chosen as standardizer and $n_1=100$.

Note: when computing an effect size estimates where standardizer is computed based on both $SD_1$ and $SD_2$, the correlation between standardizer and $\bar{X_1}-\bar{X_2}$ is null.

# When two samples are extracted from distributions with identical shapes, with **$\sigma_1 \neq \sigma_2$** and **$n_1=n_2$**

We previously mentioned that when distributions are right-skewed, $cor(SD_1,\bar{X_1}-\bar{X_2})>0$ and $cor(SD_1,\bar{X_1}-\bar{X_2})<0$. As long as $\sigma_1=\sigma_2$, $|cor(SD_1,\bar{X_1}-\bar{X_2})|=|cor(SD_2,\bar{X_1}-\bar{X_2})|$. However, when population variances are unequal across groups, the correlation between $\bar{X_1}-\bar{X_2}$ and the larger $SD$ will be much larger than the correlation between $\bar{X_1}-\bar{X_2}$ and the lower $SD$. As a consequence, when pooling both $SD$ estimates, the sign of $cor(pooled_SD,\bar{X_1}-\bar{X_2})$ wil be the same as the sign of the correlation between $\bar{X_1}-\bar{X_2}$ and the larger SD.












































```{r sdsampl,echo=FALSE}
#  Normal distribution
nSims=100000
sdsampl1<-function(SD){
  sd=rep(0,nSims)
  for (i in 1:nSims){
    
    n=20
    y1 <- rnorm(n,m=2,sd=SD)
    y2 <- rnorm(n,m=2,sd=SD)

    sd1[i]=sd(y1)    
    sd2[i]=sd(y2)    

  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

#  Symmetric distribution with high kurtosis
sdsampl2<-function(SD){
  sd=rep(0,nSims)
  skew=0
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y <- rpearson(n,moments=c(m1=2,SD^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    sd[i]=sd(y)    
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

#  Right-skewed distribution
sdsampl3<-function(SD){
  sd=rep(0,nSims)
  skew=6.32
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y <- rpearson(n,moments=c(m1=2,SD^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    sd[i]=sd(y)    
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

#  Left-skewed distribution
sdsampl4<-function(SD){
  sd=rep(0,nSims)
  skew=-6.32
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y <- rpearson(n,moments=c(m1=2,SD^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    sd[i]=sd(y)    
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

rep1 <-  sapply(seq(1,10,1),sdsampl1) # normal
rep2 <-  sapply(seq(1,10,1),sdsampl2) # pas normal
rep3 <-  sapply(seq(1,10,1),sdsampl3)# right-skewed
rep4 <-  sapply(seq(1,10,1),sdsampl4)# left-skewed
```

```{r sdsampl,echo=FALSE}
#  Normal distribution
nSims=10000
sdsampl1<-function(SD){
  sd=rep(0,nSims)
  meandiff=rep(0,nSims)
  for (i in 1:nSims){
    
    n=20
    y <- rnorm(n,m=2,sd=SD)
    sd[i]=sd(y)   
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

#  Symmetric distribution with high kurtosis
sdsampl2<-function(SD){
  sd=rep(0,nSims)
  skew=0
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y <- rpearson(n,moments=c(m1=2,SD^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    sd[i]=sd(y)    
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

#  Right-skewed distribution
sdsampl3<-function(SD){
  sd=rep(0,nSims)
  skew=6.32
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y <- rpearson(n,moments=c(m1=2,SD^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    sd[i]=sd(y)    
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

#  Left-skewed distribution
sdsampl4<-function(SD){
  sd=rep(0,nSims)
  skew=-6.32
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y <- rpearson(n,moments=c(m1=2,SD^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    sd[i]=sd(y)    
  }
  
  return(c(mean(sd),sd(sd),skewness(sd),kurtosis(sd)))
  
}

rep1 <-  sapply(seq(1,10,1),sdsampl1) # normal
rep2 <-  sapply(seq(1,10,1),sdsampl2) # pas normal
rep3 <-  sapply(seq(1,10,1),sdsampl3)# right-skewed
rep4 <-  sapply(seq(1,10,1),sdsampl4)# left-skewed
```

It is interesting to study the sampling distribution of s (and more specifically, its variability), as a function of $\sigma$, for different kind of distributions. Of course, the larger $\sigma$, the larger the standard deviation of the sampling distribution of $s$. In left plot in Figure \ref{fig:sdsamplbis}, we plotted $sd(s)$ against $\sigma$ for different kind of distributions underlying the data. We can also see that when distributions are normal, the sampling distribution of $s$ is less variable than when distributions are symmetric with high kurtosis, and the last configuration results in less variable sampling distribution of $s$ than when both skewness and kurtosis are high. However, right plot in Figure \ref{fig:sdsamplbis} reveals that for all distributions underlying the data, the standard deviation of s always varies in proportion to $\sigma$. Indeed, when plotting $\frac{sd(s)}{\sigma}$ against $\sigma$, we can see a null slope for all distributions. 

```{r sdsamplbis,fig.cap = "standard deviation of the sampling distribution of s, as a function of sigma, for difference distributions underlying the data",echo=FALSE}
par(mar=c(4,4,0,2))
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

plot(1,col="white",main="",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=c(expression(paste(gamma,1,"=0 and ",gamma,2,"=0")),expression(paste(gamma,1,"=0 and ",gamma,2,"=95.75")),expression(paste(gamma,1,"=6.32 and ",gamma,2,"=95.75")),expression(paste(gamma,1,"=-6.32 and ",gamma,2,"=95.75"))),col=c("black","orange","blue","green"),bty="n",lty=c(1,1,1,1))

#sd of sd
plot(0,xlim=c(1,10), ylim=c(0,10),col="white",xlab=expression(sigma),ylab="sd(S)")
axis(1,at=1:10,labels=TRUE)
lines(seq(1,10,1),rep1[2,],col="black")
points(seq(1,10,1),rep1[2,],pch=19,cex=.8,col="black")
lines(seq(1,10,1),rep2[2,],col="orange")
points(seq(1,10,1),rep2[2,],pch=19,cex=.8,col="orange")
lines(seq(1,10,1),rep3[2,],col="blue")
points(seq(1,10,1),rep3[2,],pch=19,cex=.8,col="blue")
lines(seq(1,10,1),rep4[2,],col="green")
points(seq(1,10,1),rep4[2,],pch=19,cex=.8,col="green")

plot(0,xlim=c(1,10), ylim=c(0,.7),col="white",xlab=expression(sigma),ylab=expression(paste("sd(S)/",sigma)))
axis(1,at=1:10,labels=TRUE)
lines(seq(1,10,1),rep1[2,]/seq(1,10,1),col="black")
points(seq(1,10,1),rep1[2,]/seq(1,10,1),pch=19,cex=.8,col="black")
lines(seq(1,10,1),rep2[2,]/seq(1,10,1),col="orange")
points(seq(1,10,1),rep2[2,]/seq(1,10,1),pch=19,cex=.8,col="orange")
lines(seq(1,10,1),rep3[2,]/seq(1,10,1),col="blue")
points(seq(1,10,1),rep3[2,]/seq(1,10,1),pch=19,cex=.8,col="blue")
lines(seq(1,10,1),rep4[2,]/seq(1,10,1),col="green")
points(seq(1,10,1),rep4[2,]/seq(1,10,1),pch=19,cex=.8,col="green")
```







# biais des estimateurs en fonction de la valeur de sigma:
corsampl1<-function(sigma){
  nSims=10000  
  glass1=rep(0,nSims)
  glass2=rep(0,nSims)
  cohen=rep(0,nSims)
  
  skew=6.32
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y1 <- rpearson(n,moments=c(m1=2,sigma^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    y2 <- rpearson(n,moments=c(m1=0,sigma^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    y3 <- rpearson(n/2,moments=c(m1=2,sigma^2,skewness=skew*(n/2-2)/sqrt(n/2*(n/2-1)),kurtosis=(kurt*(n/2-2)*(n/2-3)-6*(n/2-1))/((n/2)^2-1)+3))
    y4 <- rpearson(n/2,moments=c(m1=0,sigma^2,skewness=skew*(n/2-2)/sqrt(n/2*(n/2-1)),kurtosis=(kurt*(n/2-2)*(n/2-3)-6*(n/2-1))/((n/2)^2-1)+3))
    
  glass1[i]=(mean(y1)-mean(y2))/sd(y1)
  glass2[i]=(mean(y1)-mean(y2))/sd(y2)
  sd_pooled=sqrt((var(y3)+var(y4))/2)
  cohen[i]= (mean(y3)-mean(y4))/sd_pooled
  
  }
  
  glass_theo=2/sigma  
  relbias_glass1 <- (mean(glass1)-glass_theo)/glass_theo
  relbias_glass2 <- (mean(glass2)-glass_theo)/glass_theo
  
  cohen_theo=2/sigma
  relbias_cohen <- (mean(cohen)-cohen_theo)/cohen_theo
  
  return(c(relbias_glass1,relbias_glass2,relbias_cohen,sigma))
  }

rep=sapply(seq(1,10,1),corsampl1)
#n1=n2, sigma1=sigma2,
plot(rep[4,],rep[1,],ylim=c(0,6),xlab="sigma",ylab="biais relatif",main="right-skewed") # plus sigma est grand, pire est le biais relatif
lines(rep[4,],rep[2,],ylim=c(0,6)) # plus sigma est grand, pire est le biais relatif
lines(rep[4,],rep[3,],ylim=c(0,6),col="red") # plus sigma est grand, pire est le biais relatif

corsampl2<-function(sigma){
  nSims=10000  
  glass1=rep(0,nSims)
  glass2=rep(0,nSims)
  cohen=rep(0,nSims)
  
  skew=-6.32
  kurt=95.75
  for (i in 1:nSims){
    
    n=20
    y1 <- rpearson(n,moments=c(m1=2,sigma^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    y2 <- rpearson(n,moments=c(m1=0,sigma^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
    y3 <- rpearson(n/2,moments=c(m1=2,sigma^2,skewness=skew*(n/2-2)/sqrt(n/2*(n/2-1)),kurtosis=(kurt*(n/2-2)*(n/2-3)-6*(n/2-1))/((n/2)^2-1)+3))
    y4 <- rpearson(n/2,moments=c(m1=0,sigma^2,skewness=skew*(n/2-2)/sqrt(n/2*(n/2-1)),kurtosis=(kurt*(n/2-2)*(n/2-3)-6*(n/2-1))/((n/2)^2-1)+3))
    
  glass1[i]=(mean(y1)-mean(y2))/sd(y1)
  glass2[i]=(mean(y1)-mean(y2))/sd(y2)
  sd_pooled=sqrt((var(y3)+var(y4))/2)
  cohen[i]= (mean(y3)-mean(y4))/sd_pooled
  
  }
  
  glass_theo=2/sigma  
  relbias_glass1 <- (mean(glass1)-glass_theo)/glass_theo
  relbias_glass2 <- (mean(glass2)-glass_theo)/glass_theo
  
  cohen_theo=2/sigma
  relbias_cohen <- (mean(cohen)-cohen_theo)/cohen_theo
  
  return(c(relbias_glass1,relbias_glass2,relbias_cohen,sigma))
  }
rep2=sapply(seq(1,10,1),corsampl2)
#n1=n2, sigma1=sigma2,
plot(rep2[4,],rep2[1,],ylim=c(0,6),xlab="sigma",ylab="biais relatif",main="left-skewed") # plus sigma est grand, pire est le biais relatif
lines(rep2[4,],rep2[2,],ylim=c(0,6)) # plus sigma est grand, pire est le biais relatif
lines(rep2[4,],rep2[3,],ylim=c(0,6),col="red") # plus sigma est grand, pire est le biais relatif



# pour savoir si le cohen chnge la donne √† cause du n, j'utilise un n deux fois plus petit pour la mesure de Cohen que pour les autres. √ßa me permet de voir que si seul le sigma intervenait, le biais relatif de Cohen serait tj situ√© entre glass 1 et glass 2.... mais il y a d'autres √©l√©ments qui jouent!


refaire une simulation √† part avec un cohen qui utilise des n deux fois plus petits et comparer avec le biais relatif des deux glass
