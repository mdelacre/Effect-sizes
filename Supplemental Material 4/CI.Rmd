---
title             : "Reminder about Confidence Intervals"
shorttitle        : "CI REMINDER"

author: 
  - name          : "Marie Delacre"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "marie.delacre@ulb.ac.be"

affiliation:
  - id            : "1"
    institution   : "ULB"

authornote: |

abstract: |

keywords          : "keywords"
wordcount         : "X"
bibliography      : ["r-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("moments")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

### Introduction: How to compute a confidence interval around $\mu_1-\mu_2$ 

Considering the link between confidences intervals and NHST approach, we can think of confidence limits as the most extreme values of $\mu_1-\mu_2$ that we could define as null hypothesis and that would not lead to rejecting the null hypothesis [@Cumming_Finch_2001], that is to say, the value associated with a *p*-value that exactly equals $\frac{alpha}{2}$. 

Under the assumption of iid normal distribution of residuals with equal population variances across groups, in order to test the null hypothesis that $\mu_1-\mu_2= (\mu_1-\mu_2)_0$, we can compute the following quantity:

\begin{equation} 
t_{Student}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
(\#eq:tstudent)
\end{equation} 

With $SE = \sigma_{pooled} \times \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$ and $\sigma_{pooled} = \sqrt{\frac{(n_1-1) \times S^2_1+(n_2-1)\times S^2_2}{n_1+n_2-2}}$. 

Under the null hypothesis, this quantity will follow a central *t*- distribution with $n_1+n_2-2$ degrees of freedom. \footnote{Distribution is central because under the null hypothesis, the quantity is a (supposed normal) centered variable, divided by SE, an independant variable closely related with the $\chi^2$. }. The central *t*-distribution is always centered around $(\mu_1-\mu_2)_0$ (e.g. when we expect that $\mu_1=\mu_2$ under the null hypothesis, the central *t*-distribution is centered around 0, when we expect that $\mu_1-\mu_2=3$ under the null hypothesis, the central *t*-distribution is centered around 3, etc.). 

Considering all these information, we can therefore define $(\mu_1-\mu_2)_L$, the lower limit of the confidence interval, such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}$ exactly equals the quantile  (1-$\frac{\alpha}{2}$) of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_L$ (i.e. the symmetric *t*-distribution that is centered around $(\mu_1-\mu_2)_L$) and the upper limit $(\mu_1-\mu_2)_U$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}$ exactly equals the quantile  $\frac{\alpha}{2}$ of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_U$ (i.e. the symmetric *t*-distribution that is centered around $(\mu_1-\mu_2)_U$):

\begin{equation} 
Pr[t_{n_1+n_2-2} \geq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit1)
\end{equation} 

\begin{equation} 
Pr[t_{n_1+n_2-2} \leq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit2)
\end{equation} 

This is illustrated in Figure \ref{fig:ILLUSTRATION}.

```{r, ILLUSTRATION, fig.cap= "Sampling distribution. Both curves have the shape of a t distribution (df=n1+n2-2). Left curve is placed so that the proportion of estimates larger than the empirical mean difference (dark grey area) exactly equals alpha/2 and the right curve is placed so that the proportion of estimates lower than the empirical mean difference (light grey area) also exactly equals alpha/2."}
meandiff.CI <- function(Group.1, Group.2,conf.level=.95){ # alternative="two.sided", less or "greater"
  n1 <- length(Group.1)
  n2 <- length(Group.2)
  mean1 <- mean(Group.1)
  mean2 <- mean(Group.2)
  sd1 <- sd(Group.1)
  sd2 <- sd(Group.2)

      meandiff <- mean1-mean2
      pooled_sd <- sqrt(((n1-1)*sd1^2+(n2-1)*sd2^2)/(n1+n2-2))
      SE <- pooled_sd*sqrt(1/n1+1/n2) # standard error
      
      # Lower limit = limit of mu1-mu2 such as 1-pt(q=t_obs, df=n1+n2-2) = (1-conf.level)/2 = alpha/2
      # with t_obs = ((mean1-mean2)-(theo_mudiff))/SE 
      
      f=function(theo_mudiff,rep) 1-pt(q=(meandiff-theo_mudiff)/SE, df=n1+n2-2)-rep
      out=uniroot(f,c(0,2),rep=(1-conf.level)/2,extendInt = "yes")
      theo_mudiff.1 <- out$root

      # upper limit = limit of mu1-mu2 such as pt(q=t_obs, df=n1+n2-2) = (1-conf.level)/2 = alpha/2
      # with t_obs = ((mean1-mean2)-(theo_mudiff))/SE 
      
      f=function(theo_mudiff,rep) pt(q=(meandiff-theo_mudiff)/SE, df=n1+n2-2)-rep
      out=uniroot(f,c(0,2),rep=(1-conf.level)/2,extendInt = "yes")
      theo_mudiff.2 <- out$root
      
low_t <-(meandiff-theo_mudiff.1)/SE#+meandiff
up_t <-(meandiff-theo_mudiff.2)/SE#+meandiff

x <- seq(-5,10,.001)
y <- dt(x,df=n1+n2-2)
y2 <- dt(x-theo_mudiff.1,df=n1+n2-2)
y3 <- dt(x-theo_mudiff.2,df=n1+n2-2)
plot(0,0,col="white",xlim=c(-5,10),ylim=c(0,.5),bty="n",xaxt="n",xlab="")
lines(x+theo_mudiff.1,y)
abline(v=theo_mudiff.1,lty=2,col="black")
lines(x+theo_mudiff.2,y,col="black",cex=.1,pch=9)
abline(v=theo_mudiff.2,lty=2,col="black")
abline(v=meandiff)
abline(h=0)
axis(side=1,at=c(theo_mudiff.1,meandiff,theo_mudiff.2),labels=c(expression(paste("( ",bar(mu[1])," - ",bar(mu[2])," )"["L"])),expression(paste(bar("X"[1])," - ",bar("X"[2]))),expression(paste("( ",bar(mu[1])," - ",bar(mu[2])," )"["U"]))),col="white",gap.axis=5,cex.axis=.8,col.axis="black")
polygon(c(x[x>=meandiff],meandiff),c(y2[x>=meandiff],0), col="darkgrey")
polygon(c(x[x<=meandiff],meandiff),c(y3[x<=meandiff],0), col="lightgrey")


}

Group.1 <- rnorm(40,2,5)
Group.2 <- rnorm(40,0,5)
meandiff.CI(Group.1, Group.2,conf.level=.95)
```

Under the assumption of iid normal distributions of residuals with unequal variances across groups, in order to test the null hypothesis that $\mu_1-\mu_2= (\mu_1-\mu_2)_0$, we can compute the following quantity:

\begin{equation} 
t_{Welch}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
(\#eq:twelch)
\end{equation} 

With $SE = \sqrt{\frac{S^2_1}{n1}+\frac{S^2_2}{n2}}$. Again, under the null hypothesis, we know that this quantity will follow  a central *t*- distribution with $DF=\frac{(\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2})^2}{\frac{(\frac{S^2_1}{n_1})^2}{n_1-1}+\frac{(\frac{S^2_2}{n_2})^2}{n_2-1}}$ degrees of freedom (see Figure \ref{fig:SAMPLMEANDIFF2}). We can therefore easily define $(\mu_1-\mu_2)_L$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}$ exactly equals the quantile  (1-$\frac{\alpha}{2}$) of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_L$, and the upper limit $(\mu_1-\mu_2)_U$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}$ exactly equals the quantile  $\frac{\alpha}{2}$ of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_U$:

\begin{equation} 
Pr[t_{DF} \geq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit1)
\end{equation} 

\begin{equation} 
Pr[t_{DF} \leq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit2)
\end{equation} 

It is not the most conventional way of computing confidences limits around any mean differences, however this approach is interesting as it helps to understand how to compute confidence limits around a measure of effect size.

### How to compute a confidence interval around Cohen's $\delta$

```{r, SAMPLMEANDIFF3, fig.cap= "Sampling distribution of centered mean difference divided by SE (in grey, i.e. pivotal quantity) and not centered mean difference divided by SE (in red), assuming normality and homoscedasticity."}
samplingtest3 <- function(n1,n2,mu1,mu2,sigma,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=mu1,sd=sigma)
    B <- rnorm(n2,mean=mu2,sd=sigma) 
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else { 
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
  pooled_sd <- sqrt(((n1-1)*sd1^2+(n2-1)*sd2^2)/(n1+n2-2))
  pq <- (av-(mu1-mu2))/(pooled_sd*sqrt(1/n1+1/n2)) # *1*: centered variable, divided by SE 
  plot(density(pq),xlim=c(-15,15),col="grey",main="Sampling distribution \n (not) centered variable divided by SE",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  
  # symmetric around 0
  lines(seq(-100,100,.01),dt(seq(-100,100,.01),df=n1+n2-2),lty=2)
  
  q2 <- (av)/(pooled_sd*sqrt(1/n1+1/n2)) # *2*: NOT centered variable, divided by SE
  lines(density(q2),lty=2,col="red",lwd=1.5) # plotting the sampling distribution of the pivotal quantity
  round(skewness(q2),3)  # not symmetric around mean(q2)
  
  pooled_sigma <-  sigma
  delta <- (mu1-mu2)/pooled_sigma
  NCP <- delta*sqrt((n1*n2)/(n1+n2))
  
  lines(density(rt(1000000,df=n1+n2-2,ncp=NCP)),lty=2)
  
  legend("top",col=c("grey","red"),legend=c(paste0("centered variable divided by SE (skewness=",round(skewness(pq),2),")"),paste0("not centered variable divided by SE (skewness=",round(skewness(q2),2),")")),lty=c(2,2),bty="n")
}

samplingtest3(n1=5,n2=12, mu1=6,mu2=2,sigma=5,nsim=1000)
```

We previously mentioned that if the null hypothesis is true,$t_{Student}$ (see equation \@ref(eq:tstudent)) will follow a central *t*-distribution. However, if the null hypothesis is false, the distribution of this quantity will not be centered, and noncentral *t*-distribution will arise [@Cumming_Finch_2001], as illustrated in Figure \ref{fig:SAMPLMEANDIFF3}. 

Noncentral *t*-distributions are described by two parameters: degrees of freedom (df) and noncentrality parameter [that we will call $\Delta$; @Cumming_Finch_2001], the last being a function of $\delta$ and sample sizes $n_1$ and $n_2$:
 
\begin{equation}
\Delta = \frac{\mu_1-\mu_2}{\sigma_{pooled}} \times \sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}
(\#eq:ncp)
\end{equation} 

Considering the link between $\Delta$ and $\delta$, it is possible to compute confidence limits for $\Delta$, and divide them by $\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}$ in order to have confidence limits for $\delta$. In other word, we first need to determine the noncentrality parameters of the *t*-distributions for which $t_{Student}$ corresponds respectively to  the $1-\frac{\alpha}{2}$ and to the $\frac{\alpha}{2}$ th. quantile: 

$$P[t_{df, \Delta_L} \geq t_{Student}] = \frac{\alpha}{2} $$ 

$$P[t_{df, \Delta_U} \leq t_{Student}] = \frac{\alpha}{2} $$ 

With $df = n_1+n_2-2$. Second, we divide $\Delta_L$ and $\Delta_U$ by $\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}$ in order to define $\delta_L$ and $\delta_U$: 

$$\delta_L = \frac{\Delta_L}{\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}}$$

$$\delta_U = \frac{\Delta_U}{\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}}$$

# How to determine the confidence interval around Shieh's $\delta*$

```{r, include=FALSE}
samplingtest4 <- function(n1,n2,mu1,mu2,sigma1,sigma2,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=mu1,sd=sigma1)
    B <- rnorm(n2,mean=mu2,sd=sigma2) 
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else { 
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
  pq <- (av-(mu1-mu2))/sqrt(sd1^2/n1+sd2^2/n2)
  plot(density(pq),xlim=c(-15,15),col="grey",main="Sampling distribution \n (not) centered variable divided by SE",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  
  DF <- (sd1^2/n1 + sd2^2/n2)^2 / ((sd1^2/n1)^2/(n1-1) + (sd2^2/n2)^2/(n2-1))  
  # symmetric around 0
  
  DF_theo <- (sigma1^2/n1 + sigma2^2/n2)^2 / ((sigma1^2/n1)^2/(n1-1) + (sigma2^2/n2)^2/(n2-1))  
  lines(seq(-100,100,.01),dt(seq(-100,100,.01),df=DF_theo),lty=2)
  
  q2 <- (av)/sqrt(sd1^2/n1+sd2^2/n2) # *2*: NOT centered variable, divided by SE
  lines(density(q2),lty=2,col="red",lwd=1.5) # plotting the sampling distribution of the pivotal quantity
  round(skewness(q2),3)  # not symmetric around mean(q2)

  delta <- (mu1-mu2)/sqrt(sigma1^2/(n1/(n1+n2))+sigma2^2/(n2/(n1+n2))) # theoretical shieh
  NCP <- delta*sqrt(n1+n2)
  
#  lines(density(rt(1000000,df=DF_theo,ncp=NCP)),lty=2)
  
  legend("top",col=c("grey","red"),legend=c(paste0("centered variable divided by SE (skewness=",round(skewness(pq),3),")"),paste0("not centered variable divided by SE (skewness=",round(skewness(q2),3),")")),lty=c(2,2),bty="n")
}
```

Like $t_{Student}$, $t_{Welch}$ (see equation \@ref(eq:twelch)) will follow a central *t*-distribution only if the null hypothesis is true. If the null hypothesis is false, it will follow a noncentral *t*-distribution, as illustrated in Figure \ref{fig:SAMPLMEANDIFF4}. 

```{r, SAMPLMEANDIFF4, fig.cap= "Sampling distribution of centered mean difference divided by SE (in grey, i.e. pivotal quantity) and not centered mean difference divided by SE (in red), assuming normality and homoscedasticity."}

samplingtest4(n1=5,n2=12, mu1=6,mu2=2,sigma1=5,sigma2=5,nsim=1000)

```

The noncentrality parameter $\Delta*$ is a function of $\delta*$ and total sample size $N = n_1 + n_2$ [@Shieh_2013]
 
\begin{equation}
\Delta* = \frac{\mu_1-\mu_2}{\sqrt{\frac{\sigma_1^2}{n_1/N}+\frac{\sigma_2^2}{n_2/N}}} \times \sqrt{N}
(\#eq:ncp)
\end{equation} 

Considering the link between $\Delta$ and $\delta$, we can compute confidence limits for $\Delta*$, and divide them by $\sqrt{N}$ in order to have confidence limits for $\delta*$. We first need to determine the noncentrality parameters of the distributions for which $t_{Welch}$ corresponds respectively to  the $1-\frac{\alpha}{2}$ and to the $\frac{\alpha}{2}$ th. quantile. 
$$P[t_{v, \Delta*_L} \geq t_{Welch}] = \frac{\alpha}{2} $$ and 
$$P[t_{v, \Delta*_U} \leq t_{Welch}] = \frac{\alpha}{2} $$. 

With *v* approximated by $\hat{v} = \frac{(\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2})^2}{\frac{(\frac{S_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{S_2^2}{n_2})^2}{n_2-1}}$ [@Shieh_2013]

Second, we divide $\Delta*_L$ and $\Delta*_U$ by $\sqrt{N}$ in order to have $\delta*_L$ and $\delta*_U$ (i.e. confidences limits for Shieh's $\delta*$). 



