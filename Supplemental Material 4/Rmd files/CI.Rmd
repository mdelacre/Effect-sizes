---
title             : "Reminder about Confidence Intervals"
shorttitle        : "CI REMINDER"

author: 
  - name          : "Marie Delacre"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "marie.delacre@ulb.be"

affiliation:
  - id            : "1"
    institution   : "ULB"

authornote: |

abstract: |

keywords          : "keywords"
wordcount         : "X"
bibliography      : ["r-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("moments")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

### Introduction: How to compute a confidence interval around $\pmb{\mu_1-\mu_2}$ 

Considering the link between confidence intervals and NHST approach, we can think of the confidence limits as the most extreme values of $\mu_1-\mu_2$ that we could define as null hypothesis and that would not lead to rejecting the null hypothesis [@Cumming_Finch_2001], that is to say, the values associated with a *p*-value that exactly equals $\frac{alpha}{2}$. 

Under the assumption of iid normal distribution of residuals with equal population variances across groups, in order to test the null hypothesis that $\mu_1-\mu_2= (\mu_1-\mu_2)_0$, we can compute the following quantity:
\begin{equation*} 
t_{Student}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
(\#eq:tstudent)
\end{equation*} 
with $SE = \sigma_{pooled} \times \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$ and $\sigma_{pooled} = \sqrt{\frac{(n_1-1) \times S^2_1+(n_2-1)\times S^2_2}{n_1+n_2-2}}$. 

Under the null hypothesis, this quantity will follow a central *t*-distribution with $n_1+n_2-2$ degrees of freedom. \footnote{Distribution is central because under the null hypothesis, the quantity is a (supposed normal) centered variable, divided by SE, an independant variable closely related with the $\chi^2$. }. The central *t*-distribution is always centered around $(\mu_1-\mu_2)_0$. For example, when we expect that $\mu_1=\mu_2$ under the null hypothesis, the central *t*-distribution is centered around 0, when we expect that $\mu_1-\mu_2=3$ under the null hypothesis, the central *t*-distribution is centered around 3. 

Considering all these information we can define $(\mu_1-\mu_2)_L$, the lower limit of the confidence interval, such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}$ exactly equals the quantile  (1-$\frac{\alpha}{2}$) of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_L$ (i.e. the symmetric *t*-distribution that is centered around $(\mu_1-\mu_2)_L$) and the upper limit $(\mu_1-\mu_2)_U$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}$ exactly equals the quantile  $\frac{\alpha}{2}$ of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_U$ (i.e. the symmetric *t*-distribution that is centered around $(\mu_1-\mu_2)_U$):
\begin{equation*} 
Pr[t_{n_1+n_2-2} \geq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit1)
\end{equation*} 
\begin{equation*} 
Pr[t_{n_1+n_2-2} \leq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit2)
\end{equation*} 
Under the assumption of iid normal distribution of residuals with unequal variances across groups, in order to test the null hypothesis that $\mu_1-\mu_2= (\mu_1-\mu_2)_0$, we can compute the following quantity:
\begin{equation*} 
t_{Welch}=\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_0}{SE}
(\#eq:twelch)
\end{equation*} 
with $SE = \sqrt{\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2}}$. Again, under the null hypothesis, we know that this quantity will follow  a central *t*-distribution with $df=\frac{(\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2})^2}{\frac{(\frac{S^2_1}{n_1})^2}{n_1-1}+\frac{(\frac{S^2_2}{n_2})^2}{n_2-1}}$ degrees of freedom. We can therefore easily define $(\mu_1-\mu_2)_L$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}$ exactly equals the quantile  (1-$\frac{\alpha}{2}$) of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_L$, and the upper limit $(\mu_1-\mu_2)_U$ such as $\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}$ exactly equals the quantile  $\frac{\alpha}{2}$ of the central *t*-distribution of the null hypothesis $H_0: \mu_1 - \mu_2 = (\mu_1-\mu_2)_U$:
\begin{equation*} 
Pr[t_{df} \geq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_L}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit1)
\end{equation*} 
\begin{equation*} 
Pr[t_{df} \leq \frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)_U}{SE}]= \frac{\alpha}{2}
(\#eq:plausiblelimit2)
\end{equation*} 
It is not the most conventional way of computing confidence limits around any mean differences, however this approach is interesting as it helps to understand how to compute confidence limits around a measure of effect size.

### How to compute a confidence interval around a point estimator

```{r, SAMPLMEANDIFF3, fig.cap= "Sampling distribution of centered mean difference divided by SE (in grey) and not centered mean difference divided by SE (in red), assuming normality and homoscedasticity."}
samplingtest3 <- function(n1,n2,mu1,mu2,sigma,nsim){
  
  for (i in seq_len(nsim)){
    A <- rnorm(n1,mean=mu1,sd=sigma)
    B <- rnorm(n2,mean=mu2,sd=sigma) 
    if (i==1){
      av <- mean(A)-mean(B)  
      sd1 <- sd(A)
      sd2 <- sd(B)
      
    } else { 
      av <- c(av,mean(A)-mean(B))
      sd1 <- c(sd1,sd(A))
      sd2 <- c(sd2,sd(B))
    }
    
  }
  
#  pooled_sd <- sqrt(((n1-1)*sd1^2+(n2-1)*sd2^2)/(n1+n2-2))
#  pq <- (av-(mu1-mu2))/(pooled_sd*sqrt(1/n1+1/n2)) # *1*: centered variable, divided by SE 
#  plot(density(pq),xlim=c(-15,15),col="grey",main="Sampling distribution \n (not) centered variable divided by # SE",xlab="",cex.main=1.5,ylim=c(0,.5)) # plotting the sampling distribution of the pivotal quantity
  
  # symmetric around 0
  plot(seq(-100,100,.01),dt(seq(-100,100,.01),df=n1+n2-2),col="grey",xlim=c(-15,15),type="l",xlab="",ylab="")
  
  #q2 <- (av)/(pooled_sd*sqrt(1/n1+1/n2)) # *2*: NOT centered variable, divided by SE
  #lines(density(q2),lty=2,col="red",lwd=1.5) # plotting the sampling distribution of the pivotal quantity
  #round(skewness(q2),3)  # not symmetric around mean(q2)
  
  pooled_sigma <-  sigma
  delta <- (mu1-mu2)/pooled_sigma
  NCP <- delta*sqrt((n1*n2)/(n1+n2))
  
  lines(density(rt(1000000,df=n1+n2-2,ncp=NCP)),col="red")
  
  #legend("top",col=c("grey","red"),legend=c(paste0("centered variable divided by SE (skewness=",round(skewness(pq),2),")"),paste0("not #centered variable divided by SE (skewness=",round(skewness(q2),2),")")),lty=c(2,2),bty="n")
}

samplingtest3(n1=5,n2=12, mu1=6,mu2=2,sigma=5,nsim=1000)
```

As illustration, we will explain how to compute a confidence interval around Cohen's $d_s$ (the explanation would be very similar for all other estimators). We previously mentioned that when the null hypothesis is true, $t_{Student}$ follows a central *t*-distribution. However, when the null hypothesis is false, the distribution of this quantity is not centered and noncentral *t*-distribution arises [@Cumming_Finch_2001], as illustrated in Figure \ref{fig:SAMPLMEANDIFF3}. 

Noncentral *t*-distributions are described by two parameters: degrees of freedom ($df$) and noncentrality parameter [that we will call $\Delta$; @Cumming_Finch_2001], the last being a function of $\delta$ and sample sizes $n_1$ and $n_2$:
\begin{equation*}
\Delta = \frac{\mu_1-\mu_2}{\sigma_{pooled}} \times \sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}
(\#eq:ncp)
\end{equation*} 
Considering the link between $\Delta$ and $\delta$, it is possible to compute confidence limits for $\Delta$ and divide them by $\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}$ in order to have confidence limits for $\delta$. In other words, we first need to determine the noncentrality parameters of the *t*-distributions for which $t_{Student}$ corresponds respectively to  the quantiles $\left(1-\frac{\alpha}{2}\right)$ and $\frac{\alpha}{2}$: 
$$P[t_{df, \Delta_L} \geq t_{Student}] = \frac{\alpha}{2} $$ 
$$P[t_{df, \Delta_U} \leq t_{Student}] = \frac{\alpha}{2} $$ 
with $df = n_1+n_2-2$. Second, we divide $\Delta_L$ and $\Delta_U$ by $\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}$ in order to define $\delta_L$ and $\delta_U$: 
$$\delta_L = \frac{\Delta_L}{\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}}$$
$$\delta_U = \frac{\Delta_U}{\sqrt{\frac{n_1 \times n_2}{n_1 + n_2}}}$$