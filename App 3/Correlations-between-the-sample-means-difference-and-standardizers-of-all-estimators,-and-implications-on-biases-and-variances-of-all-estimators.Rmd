---
title             : "Correlations between the sample means difference and standardizers of all estimators, and implications on biases and variances of all estimators"
shorttitle        : ""

author: 
  - name          : "Delacre Marie"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    address       : ""
    email         : ""

affiliation:
  - id            : "1"
    institution   : "ULB"

keywords          : "keywords"
wordcount         : "X"

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("PearsonDS")
```

# Introduction

The *d*-family effect sizes are commonly used with “between-subject” designs where individuals are randomly assigned into one of two independent groups and groups scores means are compared. The population effect size is defined as 
  
\begin{equation} 
\delta = \frac{\mu_{1}-\mu_{2}}{\sigma} 
(\#eq:Cohendelta)
\end{equation} 

where both populations follow a normal distribution with mean $\mu_j$ in the $j^{th}$ population (j=1,2) and common standard deviation $\sigma$. They exist different estimators of this population effect size, varying as a function of the chosen standardizer ($\sigma$). When the equality of variances assumption is met, $\sigma$ is estimated by pooling both samples standard deviations ($S_1$ and $S_2$):

\begin{equation} 
\sigma_{Cohen's \; d_s} = \sqrt{\frac{(n_1-1) \times S_1^2+(n_2-1) \times S_2^2}{n_1+n_2-2}}
(\#eq:Cohends)
\end{equation} 

When the equality of variances assumption is not met, we are considering three alternative estimates: 

- Using the standard deviation of the control group ($S_c$) as standardizer:   

\begin{equation} 
S_{Glass's \; d_s} = S_{c}
(\#eq:Glassds)
\end{equation} 

- Using a standardizer that takes the sample sizes allocation ratio $\left( \frac{n_1}{n_2}\right)$ into account:       

\begin{equation} 
S_{Shieh's \; d_s} = \sqrt{S_1^2/q_1+S_2^2/q_2}; \;\;\; q_j=\frac{n_j}{N} (j=1,2)
(\#eq:Shiehds)
\end{equation} 

- Or using the square root of the non pooled average of both variance estimates ($S^2_1$ and $S^2_2$) as standardizer:  

\begin{equation} 
S_{Cohen's \; d'_s} = \sqrt{\frac{\left(S^2_{1}+S^2_{2} \right)}{2}}
(\#eq:cohenprimeds)
\end{equation} 

As we previously mentioned, using these formulas implies meeting the assumption of normality. Using them when distributions are not normal will have consequences on both bias and variance of all estimators. More specifically, when samples are extracted from skewed distribution, correlations might occur between the sample means difference ($\bar{X_1}-\bar{X_2}$) and standardizers ($\sigma$). Studying when these correlations occur is the main goal of this appendix. To this end, we will distinguish 4 situations, as a function of the sample sizes ratio $\left( \frac{n_1}{n_2}=1 \; vs. \frac{n_1}{n_2}\neq1\; \right)$ and the population $SD$-ratio $\left( \frac{\sigma_1}{\sigma_2}=1 \; vs. \frac{\sigma_1}{\sigma_2}\neq1\; \right)$, but before that, we will briefly introduce the impact of correlations on the bias. 

Note that we will compute correlations using the coefficient of Spearman's $\rho$. We decided to use Spearman's $\rho$ instead of Pearson's $\rho$ because some plots revealed non-perfectly linear relations.

# How correlations between the mean difference ($\bar{X_1}-\bar{X_2}$) and standardizers affect the bias of estimators.

When distributions are right-skewed, there is a positive (negative) correlation between $S_1$ ($S_2$) and ($\bar{X_1}-\bar{X_2}$). When distributions are left-skewed, there is a negative (positive) correlation between $S_1$ ($S_2$) and $\bar{X_1}-\bar{X_2}$.  When the population mean difference $\mu_1-\mu_2$ is positive (like in our simulations), all other parameters being equal, an estimator is always less biased and variable when choosing a standardizer that is positively correlated with $\bar{X_1}-\bar{X_2}$ than when choosing an estimator that is negatively correlated with $\bar{X_1}-\bar{X_2}$. When the population mean difference is negative, the reverse is true. 

GRAPHIQUE POUR L'EXPLIQUER.

Note: I mentioned "all other parameters being equal", because it is always possible that other factors in action have an opposite effect on bias and variance in order that increasing the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ does not necessarily reduce the bias and the variance. For example, when population variances are equal across groups and sample sizes are unequal, we will see below that the lower $n_j$, the larger the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$. When the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ is positive, the smaller the sample size, the larger the positive correlation. At the same time, we know that increasing the sample size decrease the bias. This is a nice example of situations where two factors might have an opposite action on bias. 

# Correlations between the mean difference ($\bar{X_1}-\bar{X_2}$) and all standardizers

## When equal population variances are estimated based on equal sample sizes (condition a)

```{r C1,include=FALSE}

n1 <- 20
n2 <- 20
n <- n1
N <- n1+n2

corrC1=function(sd,nSims=10000,m1=1,m2=0,n,skew,kurt=95.75){
   
   sd1<-rep(0,nSims)
   sd2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(1,nSims)
   mean2<-rep(0,nSims)
   
   y=rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
   
   for (i in 1:nSims){

      y1 <- sample(y,size=n,replace=TRUE)
      y2 <- sample(y,size=n,replace=TRUE)
      sd1[i] <- sd(y1)
      sd2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(sd1,sd2,meandiff,mean1,mean2))
}

C1_sym <- corrC1(sd=2,n=n,skew=0)
C1_Rs <- corrC1(sd=2,n=n,skew=6.32)
C1_Ls <- corrC1(sd=2,n=n,skew=-6.32)
```

```{r corC1sym,echo=FALSE}
C1_sym_mean1 <- C1_sym$mean1
C1_sym_mean2 <- C1_sym$mean2
C1_sym_meandiff <- C1_sym$meandiff
C1_sym_sd1 <- C1_sym$sd1
C1_sym_sd2 <- C1_sym$sd2
C1_sym_sdCohen <- sqrt(((n1-1)*C1_sym_sd1^2+(n2-1)*C1_sym_sd2^2)/(N-2))
C1_sym_sdShieh <-  sqrt(C1_sym_sd1^2/(n1/N)+C1_sym_sd2^2/(n2/N))
C1_sym_sdCohenprime <- sqrt((C1_sym_sd1^2+C1_sym_sd2^2)/2)
```

```{r corC1Rs,echo=FALSE}
C1_Rs_mean1 <- C1_Rs$mean1
C1_Rs_mean2 <- C1_Rs$mean2
C1_Rs_meandiff <- C1_Rs$meandiff
C1_Rs_sd1 <- C1_Rs$sd1
C1_Rs_sd2 <- C1_Rs$sd2
C1_Rs_sdCohen <- sqrt(((n1-1)*C1_Rs_sd1^2+(n2-1)*C1_Rs_sd2^2)/(N-2))
C1_Rs_sdShieh <-  sqrt(C1_Rs_sd1^2/(n1/N)+C1_Rs_sd2^2/(n2/N))
C1_Rs_sdCohenprime <- sqrt((C1_Rs_sd1^2+C1_Rs_sd2^2)/2)
```

```{r corC1Ls,echo=FALSE}
C1_Ls_mean1 <- C1_Ls$mean1
C1_Ls_mean2 <- C1_Ls$mean2
C1_Ls_meandiff <- C1_Ls$meandiff
C1_Ls_sd1 <- C1_Ls$sd1
C1_Ls_sd2 <- C1_Ls$sd2
C1_Ls_sdCohen <- sqrt(((n1-1)*C1_Ls_sd1^2+(n2-1)*C1_Ls_sd2^2)/(N-2))
C1_Ls_sdShieh <-  sqrt(C1_Ls_sd1^2/(n1/N)+C1_Ls_sd2^2/(n2/N))
C1_Ls_sdCohenprime <- sqrt((C1_Ls_sd1^2+C1_Ls_sd2^2)/2)
```

```{r C1sym,fig.cap="$S_j$ as a function of $\\bar{X_j}$ (j=1,2), when samples are extracted from symmetric distributions ($\\gamma_1 = 0$)",echo=FALSE}
par(mfrow=c(1,2),mar=c(5,5,5,2))
plot(C1_sym_sd1,C1_sym_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C1_sym_sd1,C1_sym_mean1,method="spearman"),2)))
plot(C1_sym_sd2,C1_sym_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C1_sym_sd2,C1_sym_mean2,method="spearman"),2)))
```

While $\bar{X_j}$ and $S_j$ (j=1,2) are uncorrelated when samples are extracted from symmetric distributions (see Figure \ref{fig:C1sym}), there is a non-null correlation between $\bar{X_j}$ and $S_j$ when distributions are skewed (Zhang, 2007). 

```{r SDC1Rs,fig.cap="$S_j$ (j=1,2) as a function $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$; top plots)",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(C1_Rs_sd1,C1_Rs_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C1_Rs_sd1,C1_Rs_mean1,method="spearman"),2)))
plot(C1_Rs_sd2,C1_Rs_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C1_Rs_sd2,C1_Rs_mean2,method="spearman"),2)))

plot(C1_Rs_sd1,C1_Rs_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Rs_sd1,C1_Rs_meandiff),2)))
plot(C1_Rs_sd2,C1_Rs_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Rs_sd2,C1_Rs_meandiff),2)))
```

More specifically, when distributions are right-skewed, there is a **positive** correlation between $\bar{X_j}$ and $S_j$ (see the two top plots in Figure \ref{fig:SDC1Rs}), resulting in a *positive* correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ and in a *negative* correlation between $S_2$ and $\bar{X_1}-\bar{X_2}$ (see the two bottom plots in Figure \ref{fig:SDC1Rs}). This can be explained by the fact that $\bar{X_1}$ and $\bar{X_1}-\bar{X_2}$ are positively correlated while $\bar{X_2}$ and $\bar{X_1}-\bar{X_2}$ and negatively correlated (of course, correlations would be trivially reversed if we computed $\bar{X_2}-\bar{X_1}$ instead of $\bar{X_1}-\bar{X_2}$).

```{r StdzrC1Rs,fig.cap="$S_{Glass's \\; d_s}$, $S_{Shieh's \\; d_s}$ and $S_{Cohen's \\; d'_s}$ as a function of the means difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(C1_Ls_sdCohen,C1_Ls_meandiff,ylab=expression(S["Cohen d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Ls_sdCohen,C1_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C1_Ls_sdShieh,C1_Ls_meandiff,ylab=expression(S["Shieh d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Ls_sdShieh,C1_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C1_Ls_sdCohenprime,C1_Ls_meandiff,ylab=expression(S["Cohen d'"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Ls_sdCohenprime,C1_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
```

One should also notice that both correlations between $S_j$ and $\bar{X_1}-\bar{X_2}$ are equal, in absolute terms (possible tiny differences might be observed due to sampling error in our simulations). As a consequence, when computing a standardizer taking both $S_1$ and $S_2$ into account, it results in a standardizer that is uncorrelated with $\bar{X_1}-\bar{X_2}$ (see Figure \ref{fig:StdzrC1Rs}).

```{r SDC1Ls,fig.cap="$S_j$ (j=1,2) as a function $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$; top plots)",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(C1_Ls_sd1,C1_Ls_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C1_Ls_sd1,C1_Ls_mean1,method="spearman"),2)))
plot(C1_Ls_sd2,C1_Ls_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C1_Ls_sd2,C1_Ls_mean2,method="spearman"),2)))

plot(C1_Ls_sd1,C1_Ls_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Ls_sd1,C1_Ls_meandiff),2)))
plot(C1_Ls_sd2,C1_Ls_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Ls_sd2,C1_Ls_meandiff),2)))
```

On the other hand, when distributions are left-skewed, there is a **negative** correlation between $\bar{X_j}$ and $S_j$ (see the two top plots in Figure \ref{fig:SDC1Ls}), resulting in a *negative* correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ and in a *positive* correlation between $S_2$ and $\bar{X_1}-\bar{X_2}$ (see the two bottom plots in Figure \ref{fig:SDC1Ls}).

```{r StdzrC1Ls,fig.cap="$S_{Glass's \\; d_s}$, $S_{Shieh's \\; d_s}$ and $S_{Cohen's \\; d'_s}$ as a function of the means difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(C1_Rs_sdCohen,C1_Rs_meandiff,ylab=expression(S["Cohen d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Rs_sdCohen,C1_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C1_Rs_sdShieh,C1_Rs_meandiff,ylab=expression(S["Shieh d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Rs_sdShieh,C1_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C1_Rs_sdCohenprime,C1_Rs_meandiff,ylab=expression(S["Cohen d'"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C1_Rs_sdCohenprime,C1_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
```

Again, because correlations between $S_j$ and $\bar{X_1}-\bar{X_2}$ are similar in absolute terms, any standardizers taking both $S_1$ and $S_2$ into account will be uncorrelated with $\bar{X_1}-\bar{X_2}$ (see Figure \ref{fig:StdzrC1Ls}).

## When equal population variances are estimated based on unequal sample sizes (condition b)

```{r C2,include=FALSE}

n1 <- 20
n2 <- 100
N <- n1+n2

corrC2=function(sd,nSims=10000,m1=1,m2=0,n1,n2,skew,kurt=95.75){
   
   sd1<-rep(0,nSims)
   sd2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   # I generate a population using N in order to be sure that both samples are extacted from equal population skewness and kurtosis
   N <- n1+n2
   y=rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  
   
   for (i in 1:nSims){

      y1 <- sample(y,size=n1,replace=TRUE)
      y2 <- sample(y,size=n2,replace=TRUE)
      sd1[i] <- sd(y1)
      sd2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(sd1,sd2,meandiff,mean1,mean2))
}

C2_sym <- corrC2(sd=2,n1=n1,n2=n2,skew=0)
C2_Rs <- corrC2(sd=2,n1=n1,n2=n2,skew=6.32)
C2_Ls <- corrC2(sd=2,n1=n1,n2=n2,skew=-6.32)

```

```{r corC2sym,echo=FALSE}
C2_sym_mean1 <- C2_sym$mean1
C2_sym_mean2 <- C2_sym$mean2
C2_sym_meandiff <- C2_sym$meandiff
C2_sym_sd1 <- C2_sym$sd1
C2_sym_sd2 <- C2_sym$sd2
C2_sym_sdCohen <- sqrt(((n1-1)*C2_sym_sd1^2+(n2-1)*C2_sym_sd2^2)/(N-2))
C2_sym_sdShieh <-  sqrt(C2_sym_sd1^2/(n1/N)+C2_sym_sd2^2/(n2/N))
C2_sym_sdCohenprime <- sqrt((C2_sym_sd1^2+C2_sym_sd2^2)/2)
```

```{r corC2Rs,echo=FALSE}
C2_Rs_mean1 <- C2_Rs$mean1
C2_Rs_mean2 <- C2_Rs$mean2
C2_Rs_meandiff <- C2_Rs$meandiff
C2_Rs_sd1 <- C2_Rs$sd1
C2_Rs_sd2 <- C2_Rs$sd2
C2_Rs_sdCohen <- sqrt(((n1-1)*C2_Rs_sd1^2+(n2-1)*C2_Rs_sd2^2)/(N-2))
C2_Rs_sdShieh <-  sqrt(C2_Rs_sd1^2/(n1/N)+C2_Rs_sd2^2/(n2/N))
C2_Rs_sdCohenprime <- sqrt((C2_Rs_sd1^2+C2_Rs_sd2^2)/2)
```

```{r corC2Ls,echo=FALSE}
C2_Ls_mean1 <- C2_Ls$mean1
C2_Ls_mean2 <- C2_Ls$mean2
C2_Ls_meandiff <- C2_Ls$meandiff
C2_Ls_sd1 <- C2_Ls$sd1
C2_Ls_sd2 <- C2_Ls$sd2
C2_Ls_sdCohen <- sqrt(((n1-1)*C2_Ls_sd1^2+(n2-1)*C2_Ls_sd2^2)/(N-2))
C2_Ls_sdShieh <-  sqrt(C2_Ls_sd1^2/(n1/N)+C2_Ls_sd2^2/(n2/N))
C2_Ls_sdCohenprime <- sqrt((C2_Ls_sd1^2+C2_Ls_sd2^2)/2)
```

When distributions are skewed, there are again non-null correlations between $\bar{X_j}$ and $S_j$, however $cor(S_1,\bar{X_1}) \neq cor(S_2,\bar{X_2})$, because of the different sample sizes. 

```{r C2corn,fig.cap="correlation between $S_j$ and $\\bar{X_j}$ when n = 25, 50, 75 or 100 and samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}

nSims=10000
m1=0
n1 <- 25
n2 <- 50
n3 <- 75
n4 <- 100
sd <- 2
kurt=95.75
skew=6.32

N <- n1+n2+n3+n4

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

sd1 <- NULL
sd2 <- NULL
sd3 <- NULL
sd4 <- NULL

y <- rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(y,size=n1,replace=TRUE)
     y2 <- sample(y,size=n2,replace=TRUE)
     y3 <- sample(y,size=n3,replace=TRUE)
     y4 <- sample(y,size=n4,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   sd1 <- c(sd1,sd(y1))
   sd2 <- c(sd2,sd(y2))
   sd3 <- c(sd3,sd(y3))
   sd4 <- c(sd4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,sd1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when n=",n1," ,Cor(mean,sd)=",round(cor(mean1,sd1,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean2,sd2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when n=",n2," ,Cor(mean,sd)=",round(cor(mean2,sd2,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean3,sd3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when n=",n3," ,Cor(mean,sd)=",round(cor(mean3,sd3,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean4,sd4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when n=",n4," ,Cor(mean,sd)=",round(cor(mean4,sd4,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          

```

```{r C2corn2,fig.cap="correlation between $S_j$ and $\\bar{X_j}$ when n = 25, 50, 75 or 100 and samples are extracted from right left distributions ($\\gamma_1 = -6.32$)",echo=FALSE}

nSims=10000
m1=0
n1 <- 25
n2 <- 50
n3 <- 75
n4 <- 100
sd <- 2
kurt=95.75
skew=-6.32

N <- n1+n2+n3+n4

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

sd1 <- NULL
sd2 <- NULL
sd3 <- NULL
sd4 <- NULL

y <- rpearson(1000000,moments=c(m1,sd^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(y,size=n1,replace=TRUE)
     y2 <- sample(y,size=n2,replace=TRUE)
     y3 <- sample(y,size=n3,replace=TRUE)
     y4 <- sample(y,size=n4,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   sd1 <- c(sd1,sd(y1))
   sd2 <- c(sd2,sd(y2))
   sd3 <- c(sd3,sd(y3))
   sd4 <- c(sd4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,sd1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when n=",n1," ,Cor(mean,sd)=",round(cor(mean1,sd1,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean2,sd2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when n=",n2," ,Cor(mean,sd)=",round(cor(mean2,sd2,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean3,sd3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when n=",n3," ,Cor(mean,sd)=",round(cor(mean3,sd3,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          
   plot(mean4,sd4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when n=",n4," ,Cor(mean,sd)=",round(cor(mean4,sd4,method="spearman"),3)),xlim=c(-0.5,2.5),ylim=c(0,15))          

```

When distributions are skewed, one observes that the larger the sample size, the lower the correlation between $S_j$ and $\bar{X_j}$ (See Figures \ref{fig:C2corn} and \ref{fig:C2corn2}).

```{r SDC2Rs,fig.cap="$S_j$ (j=1,2) as a function $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$; top plots), with n1=20 and n2=100",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(C2_Rs_sd1,C2_Rs_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C2_Rs_sd1,C2_Rs_mean1,method="spearman"),2)))
plot(C2_Rs_sd2,C2_Rs_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C2_Rs_sd2,C2_Rs_mean2,method="spearman"),2)))

plot(C2_Rs_sd1,C2_Rs_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Rs_sd1,C2_Rs_meandiff),2)))
plot(C2_Rs_sd2,C2_Rs_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Rs_sd2,C2_Rs_meandiff),2)))
```

```{r SDC2Ls,fig.cap="$S_j$ (j=1,2) as a function $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$; top plots), with n1=20 and n2=100",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(C2_Ls_sd1,C2_Ls_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C2_Ls_sd1,C2_Ls_mean1,method="spearman"),2)))
plot(C2_Ls_sd2,C2_Ls_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C2_Ls_sd2,C2_Ls_mean2,method="spearman"),2)))

plot(C2_Ls_sd1,C2_Ls_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Ls_sd1,C2_Ls_meandiff),2)))
plot(C2_Ls_sd2,C2_Ls_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Ls_sd2,C2_Ls_meandiff),2)))
```

This might explain why the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ is lower in the larger sample (See bottom plots in Figures \ref{fig:SDC2Rs} and \ref{fig:SDC2Ls}; note that with no surprise, there is a positive (negative) correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ and a negative (positive) correlation between $S_2$ and $\bar{X_1}-\bar{X_2}$ when distribution are right-skewed (left-skewed), as illustrated in the two bottom plots of Figures \ref{fig:SDC2Rs} and \ref{fig:SDC2Ls}). 

```{r StdzrC2Rs,fig.cap="$S_{Glass's \\; d_s}$, $S_{Shieh's \\; d_s}$ and $S_{Cohen's \\; d'_s}$ as a function of the means difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(C2_Rs_sdCohen,C2_Rs_meandiff,ylab=expression(S["Cohen d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Rs_sdCohen,C2_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C2_Rs_sdShieh,C2_Rs_meandiff,ylab=expression(S["Shieh d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Rs_sdShieh,C2_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C2_Rs_sdCohenprime,C2_Rs_meandiff,ylab=expression(S["Cohen d'"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Rs_sdCohenprime,C2_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
```

```{r StdzrC2Ls,fig.cap="$S_{Glass's \\; d_s}$, $S_{Shieh's \\; d_s}$ and $S_{Cohen's \\; d'_s}$ as a function of the means difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(C2_Ls_sdCohen,C2_Ls_meandiff,ylab=expression(S["Cohen d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Ls_sdCohen,C2_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C2_Ls_sdShieh,C2_Ls_meandiff,ylab=expression(S["Shieh d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Ls_sdShieh,C2_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C2_Ls_sdCohenprime,C2_Ls_meandiff,ylab=expression(S["Cohen d'"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C2_Ls_sdCohenprime,C2_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
```

This might also explain why the standardizers of Shieh's $d_s$ and Cohen's $d'_s$ are this time **correlated** with $\bar{X_1}-\bar{X_2}$ (see Figures \ref{fig:StdzrC2Rs} and \ref{fig:StdzrC2Ls}):  
- When computing $S_{Cohen's \; d'_s}$, the same weight is given to both $S_1$ and $S_2$. Therefore, it doesn't seem surprising that the sign of the correlation between $S_{Cohen's \; d'_s}$ and $\bar{X_1}-\bar{X_2}$ is the same as the size of the correlation between $\bar{X_1}-\bar{X_2}$ and the $SD$ of the smallest sample.  
- When computing $S_{Shieh's \; d_s}$, more weight is given to the $SD$ of the smallest sample, it is therefore not really surprising to observe that the correlation between $S_{Shieh's \; d_s}$ and $\bar{X_1}-\bar{X_2}$  is closer of the correlation between $S_1$ and $\bar{X_1}-\bar{X_2}$ (i.e. $cor(S_{Shieh's \; d_s},\bar{X_1}-\bar{X_2}) > cor(S_{Cohen's \; d'_s},\bar{X_1}-\bar{X_2})$)  
- When computing $S_{Cohen}$, more weight is given to the $SD$ of the largest sample,  which by compensation effect, brings the correlation very close to 0.  

The correlation $\bar{X_1}-\bar{X_2}$ and respectively $SD_1$, $SD_2$, the standardizer of Hedge's $g'_s$ and Shieh's $g_s$ and the standardizer of Hedge's $g_s$ are summarized in Table 2:


|                               |           __**population distribution**__                                  |             
|-------------------------------|:--------------------------------:|:---------------------------------------:|
|                               |      *right-skewed*              |         *left-skewed*                   |
|                               |----------------------------------|-----------------------------------------|
|        When $n_1=n_2$         | $SD_1$: *positive*               | $SD_1$: *negative*                      |
|                               | $SD_2$: *negative*               | $SD_2$: *positive*                      |
|                               | Others: *null*                   | Others: *null*                          |
|                               |                                  |                                         |
|        When $n_1>n_2$         | $SD_1$: *positive*               | $SD_1$: *negative*                      |
|                               | $SD_2$: *negative*               | $SD_2$: *positive*                      |
|                               | Others: *negative*               | Others: *positive*                      |
|                               | $S_{Cohen's \; d_s}$: *null*     | $S_{Cohen's \; d_s}$: *null*            |
|                               |                                  |                                         |
|        When $n_1<n_2$         | $SD_1$: *positive*               | $SD_1$: *negative*                      |
|                               | $SD_2$: *negative*               | $SD_2$: *positive*                      |
|                               | Others: *positive*               | Others: *negative*                      |
|                               | $S_{Cohen's \; d_s}$: *null*     | $S_{Cohen's \; d_s}$: *null*            |

Table: Correlation between standardizers ($SD_1$,$SD_2$,$S_{Cohen's \; d_s}$ and others) and $\bar{X_1}-\bar{X_2}$, when samples are extracted from skewed distributions with equal variances, as a function of the n-ratio.  


## When unequal population variances are estimated based on equal sample sizes (condition c)

```{r C3,include=FALSE}

n <- 20
n1 <- n
n2 <- n
N <- n1+n2
sd1 <- 2
sd2 <- 4


C3=function(sd1,sd2,nSims=10000,m1=1,m2=0,n,skew,kurt=95.75){
   
   SD1<-rep(0,nSims)
   SD2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   Y1=rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
   Y2=rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))
   
   for (i in 1:nSims){
      
      y1 <- sample(Y1,size=n,replace=TRUE)
      y2 <- sample(Y2,size=n,replace=TRUE)
      SD1[i] <- sd(y1)
      SD2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(SD1,SD2,meandiff,mean1,mean2))
}

C3_sym <- C3(sd1=sd1,sd2=sd2,n=n,skew=0)
C3_Rs <- C3(sd1=sd1,sd2=sd2,n=n,skew=6.32)
C3_Ls <- C3(sd1=sd1,sd2=sd2,n=n,skew=-6.32)

```

```{r corC3sym,echo=FALSE}
C3_sym_mean1 <- C3_sym$mean1
C3_sym_mean2 <- C3_sym$mean2
C3_sym_meandiff <- C3_sym$meandiff
C3_sym_sd1 <- C3_sym$SD1
C3_sym_sd2 <- C3_sym$SD2
C3_sym_sdCohen <- sqrt(((n1-1)*C3_sym_sd1^2+(n2-1)*C3_sym_sd2^2)/(N-2))
C3_sym_sdShieh <-  sqrt(C3_sym_sd1^2/(n1/N)+C3_sym_sd2^2/(n2/N))
C3_sym_sdCohenprime <- sqrt((C3_sym_sd1^2+C3_sym_sd2^2)/2)
```

```{r corC3Rs,echo=FALSE}
C3_Rs_mean1 <- C3_Rs$mean1
C3_Rs_mean2 <- C3_Rs$mean2
C3_Rs_meandiff <- C3_Rs$meandiff
C3_Rs_sd1 <- C3_Rs$SD1
C3_Rs_sd2 <- C3_Rs$SD2
C3_Rs_sdCohen <- sqrt(((n1-1)*C3_Rs_sd1^2+(n2-1)*C3_Rs_sd2^2)/(N-2))
C3_Rs_sdShieh <-  sqrt(C3_Rs_sd1^2/(n1/N)+C3_Rs_sd2^2/(n2/N))
C3_Rs_sdCohenprime <- sqrt((C3_Rs_sd1^2+C3_Rs_sd2^2)/2)
```

```{r corC3Ls,echo=FALSE}
C3_Ls_mean1 <- C3_Ls$mean1
C3_Ls_mean2 <- C3_Ls$mean2
C3_Ls_meandiff <- C3_Ls$meandiff
C3_Ls_sd1 <- C3_Ls$SD1
C3_Ls_sd2 <- C3_Ls$SD2
C3_Ls_sdCohen <- sqrt(((n1-1)*C3_Ls_sd1^2+(n2-1)*C3_Ls_sd2^2)/(N-2))
C3_Ls_sdShieh <-  sqrt(C3_Ls_sd1^2/(n1/N)+C3_Ls_sd2^2/(n2/N))
C3_Ls_sdCohenprime <- sqrt((C3_Ls_sd1^2+C3_Ls_sd2^2)/2)
```

```{r C3corn1,fig.cap="correlation between $S_j$ and $\\bar{X_j}$ when n = 25, 50, 75 or 100 and samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}

nSims=10000
m1=0
sd1 <- 2
sd2 <- 4
sd3 <- 6
sd4 <- 8
n <- 20
kurt=95.75
skew=6.32

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

SD1 <- NULL
SD2 <- NULL
SD3 <- NULL
SD4 <- NULL

Y1 <- rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y2 <- rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y3 <- rpearson(1000000,moments=c(m1,sd3^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y4 <- rpearson(1000000,moments=c(m1,sd4^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(Y1,size=n,replace=TRUE)
     y2 <- sample(Y2,size=n,replace=TRUE)
     y3 <- sample(Y3,size=n,replace=TRUE)
     y4 <- sample(Y4,size=n,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   SD1 <- c(SD1,sd(y1))
   SD2 <- c(SD2,sd(y2))
   SD3 <- c(SD3,sd(y3))
   SD4 <- c(SD4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,SD1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when sd=",sd1,",Cor(mean,sd)=",round(cor(mean1,SD1,method="spearman"),3)))          
   plot(mean2,SD2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when sd=",sd2,",Cor(mean,sd)=",round(cor(mean2,SD2,method="spearman"),3)))          
   plot(mean3,SD3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when sd=",sd3,",Cor(mean,sd)=",round(cor(mean3,SD3,method="spearman"),3)))          
   plot(mean4,SD4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when sd=",sd4,",Cor(mean,sd)=",round(cor(mean4,SD4,method="spearman"),3)))          
```

```{r C3corn2,fig.cap="correlation between $S_j$ and $\\bar{X_j}$ when n = 25, 50, 75 or 100 and samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}

nSims=10000
m1=0
sd1 <- 2
sd2 <- 4
sd3 <- 6
sd4 <- 8
n <- 20
kurt=95.75
skew=-6.32

mean1 <- NULL
mean2 <- NULL
mean3 <- NULL
mean4 <- NULL

SD1 <- NULL
SD2 <- NULL
SD3 <- NULL
SD4 <- NULL

Y1 <- rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y2 <- rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y3 <- rpearson(1000000,moments=c(m1,sd3^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  
Y4 <- rpearson(1000000,moments=c(m1,sd4^2,skewness=skew*(n-2)/sqrt(n*(n-1)),kurtosis=(kurt*(n-2)*(n-3)-6*(n-1))/(n^2-1)+3))  

  for (i in 1:nSims){
    

     y1 <- sample(Y1,size=n,replace=TRUE)
     y2 <- sample(Y2,size=n,replace=TRUE)
     y3 <- sample(Y3,size=n,replace=TRUE)
     y4 <- sample(Y4,size=n,replace=TRUE)
     
   mean1 <- c(mean1,mean(y1))
   mean2 <- c(mean2,mean(y2))
   mean3 <- c(mean3,mean(y3))
   mean4 <- c(mean4,mean(y4))

   SD1 <- c(SD1,sd(y1))
   SD2 <- c(SD2,sd(y2))
   SD3 <- c(SD3,sd(y3))
   SD4 <- c(SD4,sd(y4))

  }
   
   par(mfrow=c(2,2))
   plot(mean1,SD1,xlab=expression(bar(X)[1]),ylab=expression(S[1]),main=paste("when sd=",sd1,",Cor(mean,sd)=",round(cor(mean1,SD1,method="spearman"),3)))          
   plot(mean2,SD2,xlab=expression(bar(X)[2]),ylab=expression(S[2]),main=paste("when sd=",sd2,",Cor(mean,sd)=",round(cor(mean2,SD2,method="spearman"),3)))          
   plot(mean3,SD3,xlab=expression(bar(X)[3]),ylab=expression(S[3]),main=paste("when sd=",sd3,",Cor(mean,sd)=",round(cor(mean3,SD3,method="spearman"),3)))          
   plot(mean4,SD4,xlab=expression(bar(X)[4]),ylab=expression(S[4]),main=paste("when sd=",sd4,",Cor(mean,sd)=",round(cor(mean4,SD4,method="spearman"),3)))          
```

```{r SDC3Rs,fig.cap="$S_j$ (j=1,2) as a function $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$; top plots), with n1=20 and n2=100",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(C3_Rs_sd1,C3_Rs_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C3_Rs_sd1,C3_Rs_mean1,method="spearman"),2)))
plot(C3_Rs_sd2,C3_Rs_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C3_Rs_sd2,C3_Rs_mean2,method="spearman"),2)))

plot(C3_Rs_sd1,C3_Rs_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Rs_sd1,C3_Rs_meandiff),2)))
plot(C3_Rs_sd2,C3_Rs_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Rs_sd2,C3_Rs_meandiff),2)))
```

```{r SDC3Ls,fig.cap="$S_j$ (j=1,2) as a function $\\bar{X_j}$ (top plots) or $\\bar{X_1}-\\bar{X_2}$ (bottom plots), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$; top plots), with n1=20 and n2=100",echo=FALSE}
par(mfrow=c(2,2),mar=c(5,5,5,2))
plot(C3_Ls_sd1,C3_Ls_mean1,ylab=expression(S[1]),xlab=expression(bar(X)[1]),main=paste("rho=",round(cor(C3_Ls_sd1,C3_Ls_mean1,method="spearman"),2)))
plot(C3_Ls_sd2,C3_Ls_mean2,ylab=expression(S[2]),xlab=expression(bar(X)[2]),main=paste("rho=",round(cor(C3_Ls_sd2,C3_Ls_mean2,method="spearman"),2)))

plot(C3_Ls_sd1,C3_Ls_meandiff,ylab=expression(S[1]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Ls_sd1,C3_Ls_meandiff),2)))
plot(C3_Ls_sd2,C3_Ls_meandiff,ylab=expression(S[2]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Ls_sd2,C3_Ls_meandiff),2)))
```

When distributions are skewed, there are again non-null correlations between $\bar{X_j}$ and $S_j$. As illustrated in Figures \ref{fig:C3corn1} and \ref{fig:C3corn2}, the correlation remain the same for any population $SD$ ($\sigma$). However, the magnitude of the correlation between $S_j$ and $\bar{X_1}-\bar{X_2}$ differ: it is stronger in the sample extracted from the larger population variance. 

```{r StdzrC3Rs,fig.cap="$S_{Glass's \\; d_s}$, $S_{Shieh's \\; d_s}$ and $S_{Cohen's \\; d'_s}$ as a function of the means difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from right skewed distributions ($\\gamma_1 = 6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(C3_Rs_sdCohen,C3_Rs_meandiff,ylab=expression(S["Cohen d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Rs_sdCohen,C3_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C3_Rs_sdShieh,C3_Rs_meandiff,ylab=expression(S["Shieh d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Rs_sdShieh,C3_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C3_Rs_sdCohenprime,C3_Rs_meandiff,ylab=expression(S["Cohen d'"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Rs_sdCohenprime,C3_Rs_meandiff,method="spearman"),2)),cex.lab=1.5)
```

```{r StdzrC3Ls,fig.cap="$S_{Glass's \\; d_s}$, $S_{Shieh's \\; d_s}$ and $S_{Cohen's \\; d'_s}$ as a function of the means difference ($\\bar{X_1}-\\bar{X_2}$), when samples are extracted from left skewed distributions ($\\gamma_1 = -6.32$)",echo=FALSE}
par(mfrow=c(1,3),mar=c(5,5,5,2))
plot(C3_Ls_sdCohen,C3_Ls_meandiff,ylab=expression(S["Cohen d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Ls_sdCohen,C3_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C3_Ls_sdShieh,C3_Ls_meandiff,ylab=expression(S["Shieh d"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Ls_sdShieh,C3_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
plot(C3_Ls_sdCohenprime,C3_Ls_meandiff,ylab=expression(S["Cohen d'"[s]]),xlab=expression(paste(bar(X)[1]," - ",bar(X)[2])),main=paste("rho=",round(cor(C3_Ls_sdCohenprime,C3_Ls_meandiff,method="spearman"),2)),cex.lab=1.5)
```

This also explain that when computing a standardizer taking both $S_1$ and $S_2$ into account, it results in a standardizer that is correlated with $\bar{X_1}-\bar{X_2}$ (see Figures \ref{fig:StdzrC3Rs} and \ref{fig:StdzrC3Ls}). The correlation between the mean difference ($\bar{X_1}-\bar{X_2}$) and respectively the standardizer of Shieh's $d_s$, Cohen's $d'_s$ and Cohen's $d_s$ will have the same sign as the correlation between ($\bar{X_1}-\bar{X_2}$) and the larger $SD$. Table 1 summarizes the sign of the correlation between $\bar{X_1}-\bar{X_2}$ and respectively $SD_1$, $SD_2$ and the three standardizers taking both $SD_1$ and $SD_2$ into account (see "Others" in the Table).


|                               |           __**population distribution**__             |             
|-------------------------------|:-------------------------:|:-------------------------:|
|                               |      *right-skewed*       |         *left-skewed*     |
|                               |---------------------------|---------------------------|
|   When $\sigma_1=\sigma_2$    | $SD_1$: *positive*        | $SD_1$: *negative*        |
|                               | $SD_2$: *negative*        | $SD_2$: *positive*        |
|                               | Others: *null*            | Others: *null*            |
|                               |                           |                           |
|   When $\sigma_1>\sigma_2$    | $SD_1$: *positive*        | $SD_1$: *negative*        |
|                               | $SD_2$: *negative*        | $SD_2$: *positive*        |
|                               | Others: *positive*        | Others: *negative*        |
|                               |                           |                           |
|   When $\sigma_1<\sigma_2$    | $SD_1$: *positive*        | $SD_1$: *negative*        |
|                               | $SD_2$: *negative*        | $SD_2$: *positive*        |
|                               | Others: *negative*        | Others: *positive*        |
|                               |                           |                           |

Table: Correlation between standardizers ($SD_1$,$SD_2$ and others) and $\bar{X_1}-\bar{X_2}$, when samples are extracted from skewed distributions with equal sample sizes, as a function of the SD-ratio.  

## When unequal population variances are estimated based on unequal sample sizes (conditions d and e)

# When distributions are right-skewed


We already know that Glass's $d_s$ using $SD_1$ will have the smallest bias (and that Glass's $d_s$ using $SD_2$ as standardizer will have the largest one) when:  
- $n_1 > n_2$ (condition b)  
- $\sigma_1 < \sigma_2$ (condition c)
There is therefore no surprises that the most extreme differences between both Glass's estimators (in favour of using $SD_1$) occurs when there is a negative pairing between $n$ and $SD$, and $n_1 > n_2$. 

# When distributions are left-skewed

We already know that Glass's $d_s$ using $SD_2$ will have the smallest bias (and that Glass's $d_s$ using $SD_1$ as standardizer will have the largest one) when:  
- $n_1 < n_2$ (condition b)  
- $\sigma_1 > \sigma_2$ (condition c)
There is therefore no surprises that the most extreme differences between both Glass's estimators (in favour of using $SD_2$) occurs when there is a negative pairing between $n$ and $SD$, and $n_1 < n_2$. 


```{r Hetunbal,include=FALSE}
Hetunbal=function(sd1,sd2,nSims=10000,m1=1,m2=0,n1,n2,skew,kurt=95.75){
   
   SD1<-rep(0,nSims)
   SD2<-rep(0,nSims)
   meandiff<-rep(0,nSims)
   mean1<-rep(0,nSims)
   mean2<-rep(0,nSims)
   
   # I generate populations using N in order to be sure that both population have the exact same skeweness and kurtosis
   N <- n1+n2
   Y1=rpearson(1000000,moments=c(m1,sd1^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))  
   Y2=rpearson(1000000,moments=c(m1,sd2^2,skewness=skew*(N-2)/sqrt(N*(N-1)),kurtosis=(kurt*(N-2)*(N-3)-6*(N-1))/(N^2-1)+3))
   
   for (i in 1:nSims){
      
      y1 <- sample(Y1,size=n1,replace=TRUE)
      y2 <- sample(Y2,size=n2,replace=TRUE)
      SD1[i] <- sd(y1)
      SD2[i] <- sd(y2)
      meandiff[i] <- mean(y1)-mean(y2)
      mean1[i] <- mean(y1)
      mean2[i] <- mean(y2)
      
   }
   
   return(data.frame(SD1,SD2,meandiff,mean1,mean2))
}

Hetunbal_sym <- Hetunbal(sd1=2,sd2=2,n1=n1,n2=n2,skew=0)
Hetunbal_Rs <- Hetunbal(sd1=2,sd2=2,n1=n1,n2=n2,skew=6.32)
Hetunbal_Ls <- Hetunbal(sd1=2,sd2=2,n1=n1,n2=n2,skew=-6.32)

```
